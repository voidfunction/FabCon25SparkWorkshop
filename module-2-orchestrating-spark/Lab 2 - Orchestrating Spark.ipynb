{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a498c2-187f-41c7-bf9a-b93b6a1e0cc5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 🎼 **Lab 2 - Orchestrating Spark**\n",
    "In this module, we will explore how to orchestrate Spark workloads using Data Factory, Fabric Scheduler, and built in orchestator functions. Additionally, we will also explore how to use resource files to make code more modular.\n",
    "\n",
    "## 🎯 What You'll Learn \n",
    "\n",
    "By the end of this lab, you'll gain insights into:  \n",
    "\n",
    "- Reference Notebook via ```%run```\n",
    "- Reference Notebook via ```notebookutils.notebook.run```\n",
    "- Reference multiple Notebooks via ```notebookutils.notebook.runMultiple```\n",
    "- How to use Notebook resources\n",
    "- How to add Notebooks into pipelines\n",
    "- Running Notebooks in a High Concurrency (HC) Session\n",
    "- Scheduling notebook with the Fabric Scheduler\n",
    "- Using Notebook/Environment Resources to build and orchestrate modular code\n",
    "- Using Spark Job definition as batch job\n",
    "---\n",
    "\n",
    "**Get Ready to Code!**\n",
    "Now that you have an overview, let's get started with hands-on exercises! 🚀\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3388f879-7a7a-4c17-b351-4b3600ea652b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 🚧 **2.1 Create and Prepare Code Assets**\n",
    "### **2.1.1 Create Child Notebooks**\n",
    "Create two notebooks: `childNotebook1` and `childNotebook2`. You can create them manually or download and upload the prebuilt versions:\n",
    "- [childNotebook1.ipynb](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-2-orchestrating-spark/_lab_materials/childNotebook1.ipynb)\n",
    "- [childNotebook2.ipynb](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-2-orchestrating-spark/_lab_materials/childNotebook2.ipynb)\n",
    "\n",
    "<details> <summary><strong>🔑 childNotebook1:</strong> Click to reveal code</summary>\n",
    "\n",
    "```python\n",
    "# Code cell 1, marked as parameters cell\n",
    "parameter1 = ''\n",
    "parameter2 = ''\n",
    "\n",
    "# Code cell 2\n",
    "print(f'This is child notebook with parameter1 = {parameter1}, parameter2 = {parameter2}')\n",
    "\n",
    "# Code cell 3\n",
    "# Return the function with exit value\n",
    "notebookutils.notebook.exit(f'Exit with current Notebook Name: {mssparkutils.runtime.context[\"currentNotebookName\"]}')\n",
    "\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details> <summary><strong>🔑 childNotebook2:</strong> Click to reveal code</summary>\n",
    "\n",
    "```python\n",
    "# Code cell 1, marked as parameters cell\n",
    "input1 = ''\n",
    "input2 = ''\n",
    "\n",
    "# Code cell 2\n",
    "print(\"cell1 in childNotebook2\")\n",
    "print(f'input1 = {input1}\\ninput2 = {input2}')\n",
    "\n",
    "# Code cell 3\n",
    "# Return the function with exit value\n",
    "notebookutils.notebook.exit(f'Exit with current Notebook Name: {mssparkutils.runtime.context[\"currentNotebookName\"]}')\n",
    "```\n",
    "</details>\n",
    "\n",
    "### **2.1.2 Create Python Files**\n",
    "Now we will create a simple Python module in the Notebook built-in resource.\n",
    "\n",
    "Run the following to create a Python module called _my_module_:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be739136-5ebd-40e6-af13-3d6ff4f28ca5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "mssparkutils.fs.put(\"file:///synfs/nb_resource/builtin/observations.py\", \"\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095bacb-79d6-4272-b341-82dc8e5d698f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 🔗 **2.2 Run and Chain Notebooks**\n",
    "### **2.2.1 Inject a Notebook with _%run_**\n",
    "Use [`%run`](https://learn.microsoft.com/en-us/fabric/data-engineering/author-execute-notebook#reference-run) to inject another Notebook's code into the current session:\n",
    "\n",
    "```python\n",
    "%run childNotebook1 { 'parameter1': 'value1', 'parameter2': 'value2' }\n",
    "```\n",
    "You can also reference Python or SQL files from Notebook or Environment resource folders:\n",
    "\n",
    "```python\n",
    "%run [-b/--builtin | -e/--environment | -c/--current] script_file.py/.sql [variables ...]\n",
    "```\n",
    "\n",
    "`%run` options:\n",
    "- `-b` / `--builtin`: Built-in notebook resources\n",
    "- `-e` / `--environment`: Environment resources\n",
    "- `-c` / `--current`: Always uses the current Notebook's resources, even if the current Notebook is referenced by other Notebooks\n",
    "\n",
    "📌 **Challenge:** Use `run%` to run the code from **childNotebook2** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957c399-6d4b-4274-b4a0-e158cd0f207c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4605fb-6ac8-430e-8d25-6d18199d3e11",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "<details>\n",
    "  <summary><strong>🔑 Answer:</strong> Click to reveal</summary>\n",
    "\n",
    "```python\n",
    "%run childNotebook2 { 'input1': 'foo', 'input2': 'bar' }\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1eb8fa-c341-4145-b42d-50c72be37eca",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## **2.2.2 Run a Notebook Programmatically with _notebookutils.notebook.run_**\n",
    "The [```notebookutils.notebook.run```](https://learn.microsoft.com/en-us/fabric/data-engineering/notebook-utilities#reference-a-notebook) function references a notebook and returns its exit value. You can run nesting function calls in a notebook interactively or in a pipeline. The notebook being referenced runs on the Spark pool of the notebook that calls this function. In comparison to `%run`, this method shows up as a distinct job with a Notebook snapshot avalable in the Monitoring hub.\n",
    "\n",
    "```python\n",
    "notebookutils.notebook.run(\"notebook name\", <timeoutSeconds>, <parameterMap>, <workspaceId>)\n",
    "```\n",
    "\n",
    "![nbutils.run](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-2-orchestrating-spark/_media/Reference%20notebook%20via%20nbutils.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ebb91d-11c9-4c9b-a4af-b0f38d313a2c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### **2.2.3 Reference multi notebooks via _notebookutils.notebook.runMultiple_**\n",
    "The [`notebookutils.notebook.runMultiple`](https://learn.microsoft.com/en-us/fabric/data-engineering/notebook-utilities#reference-run-multiple-notebooks-in-parallel) function allows you to run multiple notebooks in parallel or with a predefined DAG (directed-acyclic-graph). The API executes the child notebooks similar to high-concurrency mode as the same spark session is used so that compute resources are shared.\n",
    "\n",
    "```python\n",
    "notebookutils.notebook.runMultiple([\"NotebookSimple\", \"NotebookSimple2\"])\n",
    "```\n",
    "\n",
    "📌 **Challenge:** Use `runMultiple` to run both **childNotebook1** and **childNotebook2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bbad3b-8dab-436c-a2c0-a65abc0bb702",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "348c673e-394e-4927-807b-52dbbb9cc4c6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "<details>\n",
    "  <summary><strong>🔑 Answer:</strong> Click to reveal</summary>\n",
    "\n",
    "~~~python\n",
    "exitValues = notebookutils.notebook.runMultiple([\"childNotebook1\", \"childNotebook2\"])\n",
    "print(exitValues)\n",
    "~~~\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "💡 **Tip:** you can use the `json` Python module to parse and format the exit values:\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "import json\n",
    "print(json.dumps(exitValues, indent=4))\n",
    "```\n",
    "\n",
    "![nbutils.multirun](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-2-orchestrating-spark/_media/Reference%20multi%20notebooks%20via%20nbutils.jpg?raw=true)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **2.2.3.1 Specifying a DAG for Additional Control**\n",
    "Run the below code to see an example of how you have use a DAG to control the exact sequencing and Notebook level configuration options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aec2f8-8ee9-4908-b426-1ff87cfd4a46",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "DAG = {\n",
    "    \"activities\": [\n",
    "        {\n",
    "            \"name\": \"step1\", # activity name, must be unique\n",
    "            \"path\": \"childNotebook1\", # notebook path\n",
    "            \"timeoutPerCellInSeconds\": 90, # max timeout for each cell, default to 90 seconds\n",
    "            \"args\": {\"parameter1\": \"foo\", \"parameter2\": \"bar\"}, # notebook parameters\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"step2\",\n",
    "            \"path\": \"childNotebook2\",\n",
    "            \"timeoutPerCellInSeconds\": 120,\n",
    "            \"args\": {\"input1\": \"foo\", \"input2\": \"bar\"},\n",
    "            \"dependencies\": [\"step1\"]\n",
    "        }\n",
    "    ],\n",
    "    \"timeoutInSeconds\": 43200, # max timeout for the entire DAG, default to 12 hours\n",
    "    \"concurrency\": 50 # max number of notebooks to run concurrently, default to 50, this is limited by the number of executors in your Spark Pool.\n",
    "}\n",
    "results = notebookutils.notebook.runMultiple(DAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af3177-b471-4cb2-9dc7-44a1c00dd7bb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### **2.2.4 Manual Multithreading**\n",
    "Manual multithreading puts **you** in control. Instead of relying on notebook chaining or orchestration tools, you can spin up lightweight concurrent task execution right inside your notebook using Python's built-in `concurrent.futures` module.\n",
    "\n",
    "This approach is perfect for:\n",
    "\n",
    "- 🔄 **I/O-bound operations** (e.g., API calls, file reads)\n",
    "- ⏱️ **Parallelizing lightweight tasks** without leaving the notebook\n",
    "- 🧪 Quick experiments where full orchestration would be overkill\n",
    "\n",
    "You define a function, launch it in multiple threads, and collect the results—all in the same cell. No snapshotting, no external runners, no magic, just raw code.\n",
    "\n",
    "> ⚠️ With great power comes great responsibility:  \n",
    "> Multithreading gives you raw control, but it also means managing error handling, result collection, and potential thread safety issues yourself.\n",
    "\n",
    "Used wisely, it's a powerful tool in your notebook arsenal—especially when speed and flexibility matter more than rich monitoring and debugging capabilities.\n",
    "\n",
    "Run the below code which simulates running 8 instances of a lightweight task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977664ab-0760-4cab-a66f-0719d3204dc4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "# Example job function\n",
    "def job(job_id):\n",
    "    print(f\"Starting job {job_id}\")\n",
    "    time.sleep(5)  # Simulate a task\n",
    "    print(f\"Finished job {job_id}\")\n",
    "    return f\"Job {job_id} completed.\"\n",
    "\n",
    "# Total jobs to execute\n",
    "total_jobs = 8\n",
    "# Maximum number of concurrent jobs\n",
    "max_concurrency = 8\n",
    "\n",
    "# Use ThreadPoolExecutor to manage concurrency\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit all jobs to the executor\n",
    "    futures = [executor.submit(job, job_id) for job_id in range(total_jobs)]\n",
    "    \n",
    "    # As each job completes, you can process its result (if needed) in the order they are completed\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        print(future.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c735b1c1-6e8f-4f95-9243-a76dfc38ceb2",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### **2.2.5 Data Factory Pipelines**\n",
    "The Notebook activity in pipeline allows you to run Notebook created in Microsoft Fabric. You can create a Notebook activity directly through the Fabric user interface. This article provides a step-by-step walkthrough that describes how to create a Notebook activity using the Data Factory user interface.\n",
    "\n",
    "> ⚠️ **For this section there's no interactive lab material but feel free to explore during one of the breaks.**\n",
    "\n",
    "#### **2.2.5.1 Orchestrating as Notebook Activity**\n",
    "Add notebook into new/existing pipelines:\n",
    "\n",
    "<br>\n",
    "\n",
    "![](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-2-orchestrating-spark/_media/Add%20to%20pipeline.jpg?raw=true)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **2.2.5.2 Add session tag for High Concurrency Session to reuse your sessions**\n",
    "\n",
    "<br>\n",
    "\n",
    "![](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-2-orchestrating-spark/_media/session-tag-001.png?raw=true)\n",
    "\n",
    "<br>\n",
    "\n",
    "Trigger a pipeline run:\n",
    "\n",
    "<br>\n",
    "\n",
    "![](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-2-orchestrating-spark/_media/trigger%20pipeline%20run.jpg?raw=true)\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Select the Settings tab, select an existing notebook from the Notebook dropdown, and optionally specify any parameters to pass to the notebook.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-2-orchestrating-spark/_media/Pass%20parameters%20from%20Notebook%20activity%20.jpg?raw=true)\n",
    "\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e43a2-3778-4f85-8ca1-43622e75fe5d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### **2.2.6 Four ways to run notebooks. One right choice—depending on your goal**\n",
    "\n",
    "Notebooks are powerful—but once you need to **reuse logic**, **chain executions**, or **run tasks in parallel**, you're faced with a decision:\n",
    "\n",
    "### 🧩 `%run`\n",
    "Injects and runs another notebook *inline* in the current session.\n",
    "\n",
    "### 📦 `notebookutils.notebook.run()` / `runMultiple()`\n",
    "Executes notebooks as *isolated tasks*, optionally with return values.\n",
    "\n",
    "### ⚙️ Manual Multithreading\n",
    "Uses Python’s `concurrent.futures` to run functions concurrently inside the same notebook.\n",
    "\n",
    "### 🛠️ Data Factory (DF) Pipeline\n",
    "Orchestrates notebooks *from outside*, as part of a managed pipeline with retries, dependencies, and monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "Each approach has its own superpowers—and limitations—when it comes to:\n",
    "\n",
    "- 🔁 Execution context  \n",
    "- 🛠 Parameter handling  \n",
    "- 🧠 Return values  \n",
    "- 👀 Output visibility  \n",
    "- 🔐 Variable isolation  \n",
    "- ⚡ Performance  \n",
    "- 🔥 Error handling  \n",
    "- 🔄 Parallel execution  \n",
    "\n",
    "---\n",
    "\n",
    "👇 The table below breaks it all down so you can pick the **right orchestration strategy**—whether you're building modular pipelines, triggering notebooks conditionally, or executing high-throughput tasks in parallel.\n",
    "\n",
    "\n",
    "\n",
    "| Feature / Behavior                     | `%run`                        | `notebookutils.notebook.run/runMultiple()`       | Manual Multithreading (`concurrent.futures`)                     | Data Factory Activity                                 |\n",
    "|----------------------------------------|-------------------------------|--------------------------------------------------|------------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Execution Context**                  | Inline in current session     | Separate notebook within same Spark session       | Threads within current notebook                                 | External orchestrator calling notebooks                    |\n",
    "| **Parameterization**                   | ❌ Static only                | ✅ Dynamic supported                             | ✅ Fully dynamic                                                 | ✅ Pipeline parameters passed in                            |\n",
    "| **Return Value**                       | ❌ None                       | ✅ via `exitValue`                               | ✅ via `.result()`                                               | ✅ via `exitValue` exposed in activity outputs              |\n",
    "| **Output Visibility**                  | ✅ Inline                     | ✅ Visible in Monitoring snapshot                | ❌ Hidden unless logged                                          | ✅ Visible in Monitoring snapshot               |\n",
    "| **Variable Sharing**                   | ✅ Full access                | ❌ Isolated                                      | ⚠️ Partial (globals/shared memory)                              | ❌ Fully isolated execution, even when using HC mode    |\n",
    "| **Use Case**                           | Reusing setup/config code     | Modular notebook logic                            | Optimizing compute utilization: parallel I/O, API calls, Spark tasks  | UI based orchestration w/ robust out-of-the-box features (retries, conditional logic, etc.)   |\n",
    "| **Execution Overhead**                 | 🟢 Low                        | 🟡 Medium                                        | 🟢 Ultra-Low                                                     | 🔴 Medium-High (launch, logging overhead, cross engine communication)   |\n",
    "| **Error Handling**                     | ❌ Immediate stop             | ✅ Errors captured via `exitValue`               | ⚠️ Must handle manually in threads                              | ✅ Built-in retry, errors captured via `exitValue` |\n",
    "| **Performance (Orchestration)**        | 🟢 Semi-fast                  | 🚀 Good for modular orchestration but doesn't scale well | ⚡ Fastest, particularly for I/O bound operations       | 🧱 Slower, built for reliability                            |\n",
    "| **Parallel Execution Support**         | ❌ No                         | ✅ Yes via `runMultiple()`                       | ✅ Yes via threads                                              | ✅ Yes via parallel pipeline activities |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92477ae-b8cc-4a88-8c77-1a11b3b3f3a8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### **2.2.7 Enable Notebook schedule on Notebook settings page**\n",
    "\n",
    "<br>\n",
    "\n",
    "![](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-2-orchestrating-spark/_media/Schedule%20Notebook.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489fbd46-591c-4fa3-af5f-1d08f4152c3c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## **2.3 Using Resources to Execute Python Modules**\n",
    "The notebook resource explorer provides a Unix-like file system to help you manage your folders and files. It offers a writable file system space where you can store small-sized files, such as code modules or any other code assets. You can easily access them with code in the notebook as if you were working with your local file system.\n",
    "\n",
    "> Data files generally **should not** be stored in the Resource folder, use OneLake instead. Use of data files in resources should be limited to very small files needed to run unit tests.\n",
    "\n",
    "### **2.3.1 Editing Resource Files**\n",
    "Go to **Resources** in the left-side object explorer, expand **Built-in** and select to **View and Edit** to open the `observations.py` Python file for editing on the right side of your screen.\n",
    "\n",
    "**Copy and Paste** the below code into the file editor and click the **save button**.\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, expr, explode_outer\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType\n",
    "\n",
    "class Observations:\n",
    "    def __init__(self, spark: SparkSession, raw_path: str, checkpoint_path: str, target_table: str):\n",
    "        self.spark = spark\n",
    "        self.raw_path = raw_path\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.target_table = target_table\n",
    "\n",
    "        # Schema for the JSON files\n",
    "        self.schema = StructType([\n",
    "            StructField(\"id\", StringType(), True),\n",
    "            StructField(\"resourceType\", StringType(), True),\n",
    "            StructField(\"status\", StringType(), True),\n",
    "            StructField(\"subject\", StructType([\n",
    "                StructField(\"reference\", StringType(), True)\n",
    "            ]), True),\n",
    "            StructField(\"encounter\", StructType([\n",
    "                StructField(\"reference\", StringType(), True)\n",
    "            ]), True),\n",
    "            StructField(\"category\", ArrayType(StructType([\n",
    "                StructField(\"coding\", ArrayType(StructType([\n",
    "                    StructField(\"code\", StringType(), True),\n",
    "                    StructField(\"display\", StringType(), True)\n",
    "                ])), True)\n",
    "            ])), True),\n",
    "            StructField(\"code\", StructType([\n",
    "                StructField(\"coding\", ArrayType(StructType([\n",
    "                    StructField(\"code\", StringType(), True),\n",
    "                    StructField(\"display\", StringType(), True)\n",
    "                ])), True)\n",
    "            ]), True),\n",
    "            StructField(\"effectiveDateTime\", StringType(), True),\n",
    "            StructField(\"valueQuantity\", StructType([\n",
    "                StructField(\"value\", DoubleType(), True),\n",
    "                StructField(\"unit\", StringType(), True)\n",
    "            ]), True)\n",
    "        ])\n",
    "\n",
    "    def parseRaw(self, df):\n",
    "        df = df.withColumn(\"category_array\", explode_outer(col(\"category\")))\n",
    "\n",
    "        return df.select(\n",
    "            col(\"id\"),\n",
    "            col(\"resourceType\"),\n",
    "            col(\"status\"),\n",
    "            col(\"subject.reference\").alias(\"subject_reference\"),\n",
    "            split(col(\"subject.reference\"), \"/\")[1].alias(\"patient_id\"),\n",
    "            col(\"encounter.reference\").alias(\"encounter_reference\"),\n",
    "            col(\"category_array.coding\")[0][\"code\"].alias(\"category_code\"),\n",
    "            col(\"category_array.coding\")[0][\"display\"].alias(\"category_display\"),\n",
    "            col(\"code.coding\")[0][\"code\"].alias(\"observation_code\"),\n",
    "            col(\"effectiveDateTime\"),\n",
    "            col(\"valueQuantity.value\").alias(\"value_quantity\"),\n",
    "            col(\"valueQuantity.unit\").alias(\"value_unit\")\n",
    "        )\n",
    "\n",
    "    def streamToSilver(self):\n",
    "        raw_df = self.spark.readStream.schema(self.schema).json(self.raw_path)\n",
    "\n",
    "        parsed_df = self.parseRaw(raw_df)\n",
    "\n",
    "        query = parsed_df.writeStream \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .format(\"delta\") \\\n",
    "            .trigger(availableNow=True) \\\n",
    "            .option(\"checkpointLocation\", self.checkpoint_path) \\\n",
    "            .toTable(self.target_table)\n",
    "\n",
    "        query.awaitTermination()\n",
    "\n",
    "    def updateGold(self):\n",
    "        self.spark.sql(\"\"\"\n",
    "            INSERT INTO TABLE gold.dbo.patientobservations\n",
    "            SELECT\n",
    "                o.id AS observation_id,\n",
    "                o.resourceType,\n",
    "                o.status,\n",
    "                o.patient_id,\n",
    "                o.encounter_reference,\n",
    "                \n",
    "                -- Patient details from the patient_silver table\n",
    "                p.gender,\n",
    "                p.birthDate,\n",
    "                p.deceasedDateTime,\n",
    "                p.last_name,\n",
    "                p.first_name,\n",
    "            \n",
    "                -- Observation specific details\n",
    "                o.category_code,\n",
    "                o.category_display,\n",
    "                o.observation_code,\n",
    "                o.effectiveDateTime,\n",
    "                o.value_quantity,\n",
    "                o.value_unit\n",
    "            \n",
    "            FROM silver.dbo.observations_streaming2 o\n",
    "            JOIN silver.dbo.patient p\n",
    "                ON o.patient_id = p.id\n",
    "            WHERE NOT EXISTS (SELECT DISTINCT observation_id FROM gold.dbo.patientobservations where observation_id <> o.id)\n",
    "        \"\"\")\n",
    "\n",
    "```\n",
    "\n",
    "Python modules in the Notebook and Environment resources can be imported just like any other Python module (i.e. from PyPi).\n",
    "- **Notebook Resources**: use `from builtin.<optional_folder_path>.<module_name> import *`\n",
    "- **Environment Resources**: use `from env.<optional_folder_path>.<module_name> import *`\n",
    "\n",
    "\n",
    "📌 **Challenge:** Import the `observations.py` module aliased as `observations`. You can either manually type the import statement OR drag and drop the `observations.py` module onto the Notebook canvas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defafc15-0386-4ffa-8e57-d69f1637ffb4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c914fa8-7de3-4fec-a8ca-f4551e9113ec",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "<details>\n",
    "  <summary><strong>🔑 Answer:</strong> Click to reveal</summary>\n",
    "\n",
    "~~~python\n",
    "from builtin.observations import *\n",
    "~~~\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "💡 **Tip:** Python modules, including those imported from resources can be reloaded via executing the below magic commands. _You only need to run this once_, anytime the modules classes or functions are referenced the source module files will be re-imported. This is extremely helpful while doing development and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3338b1f-6600-4b12-ad70-91490c0d715e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4257f5b6-0e30-4ac1-97ac-4f0e0d4011d4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Now run the below to initialize an instance of the `observations` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6fd8f107-7367-4ba3-a4a9-bee9fc91ee4c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-25T21:02:14.1317386Z",
       "execution_start_time": "2025-03-25T21:02:13.8606185Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "63cfcd70-445f-4e16-bfca-b7c932914a58",
       "queued_time": "2025-03-25T21:02:13.859453Z",
       "session_id": "6aed272a-34d4-40b2-8f31-ffd972952b29",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 37,
       "statement_ids": [
        37
       ]
      },
      "text/plain": [
       "StatementMeta(, 6aed272a-34d4-40b2-8f31-ffd972952b29, 37, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observations_elt = observations.Observations(\n",
    "    spark=spark,\n",
    "    raw_path=\"Files/observationsraw\",\n",
    "    checkpoint_path=f\"abfss://{notebookutils.runtime.context['currentWorkspaceName']}@{spark.conf.get('fs.defaultFS').split('@')[1]}silver.Lakehouse/Files/checkpoints/observations/\",\n",
    "    target_table=\"silver.dbo.observations_streaming\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2812cd-d792-4b56-977f-4376512f457b",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "To perform **actions** on this class we can now call methods (functions relating to the class) to perform our batch streaming job to process new data into silver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29a7a32e-51af-4961-be5d-8cff3d2226d7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-25T21:02:25.7936504Z",
       "execution_start_time": "2025-03-25T21:02:18.0534948Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "2a824368-649d-443f-a041-130288747c5d",
       "queued_time": "2025-03-25T21:02:18.0522084Z",
       "session_id": "6aed272a-34d4-40b2-8f31-ffd972952b29",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 38,
       "statement_ids": [
        38
       ]
      },
      "text/plain": [
       "StatementMeta(, 6aed272a-34d4-40b2-8f31-ffd972952b29, 38, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observations_elt.streamToSilver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e7939-eae7-4f88-9a5f-72185022930c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "We can do the same to incrementally update our gold _patientobservations_ table with the new data from silver.\n",
    "\n",
    "📌 **Challenge:** call the `updateGold()` method from the `observations_elt` class instance to update the _patientobservations_ table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6d373-f999-40fc-9965-7d289b1133e8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bc0d867-a6ef-4fcc-a289-2c513b565a92",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "<details>\n",
    "  <summary><strong>🔑 Answer:</strong> Click to reveal</summary>\n",
    "\n",
    "~~~python\n",
    "observations_elt.updateGold()\n",
    "~~~\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09618fd8",
   "metadata": {},
   "source": [
    "## **2.4 Create an Apache Spark job definition in Fabric**\n",
    "To create a Spark job definition for PySpark:\n",
    "1. Download the sample Parquet file [yellow_tripdata_sampledata.parquet](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-2-orchestrating-spark/_lab_materials/yellow_tripdata_sampledata.parquet) and upload it to the files section of the lakehouse.\n",
    "1. Create a new Spark job definition.\n",
    "1. Select **PySpark (Python)** from the **Language** dropdown.\n",
    "1. Download the [createTablefromParquet.py](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-2-orchestrating-spark/_lab_materials/createTablefromParquet.py) sample and upload it as the main definition file. The main definition file (*job.Main*) is the file that contains the application logic and is mandatory to run a Spark job. For each Spark job definition, you can only upload one main definition file.\n",
    "1. Provide command line arguments for the job, if needed. Use a space as a splitter to separate the arguments.\n",
    "1. Add the lakehouse reference to the job. You must have at least one lakehouse reference added to the job. This lakehouse is the default lakehouse context for the job.\n",
    "\n",
    "![](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-2-orchestrating-spark/_media/run%20sjd%20for%20debug.jpg?raw=true)\n",
    "\n",
    "### **2.4.1 Enable Spark job definition schedule on settings page**\n",
    "\n",
    "<br>\n",
    "\n",
    "![](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-2-orchestrating-spark/_media/run%20sjd%20for%20schedule.jpg?raw=true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc711ea-90ad-4e7a-a5df-f70a5a9cbf57",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 🎉 Wrapping Up the Exercise: Orchestrating Spark\n",
    "\n",
    "Congrats on completing this hands-on exercise where we covered various options to orchestrate your Spark jobs, from pro-code to no-code, plenty of options exist to build robust data solutions."
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "9308f974-121a-4491-9d8a-8dde47a9ce9b",
    "default_lakehouse_name": "bronze",
    "default_lakehouse_workspace_id": "d24c0aa4-e6b2-4571-8e9a-6ae82ebf926d",
    "known_lakehouses": [
     {
      "id": "9308f974-121a-4491-9d8a-8dde47a9ce9b"
     },
     {
      "id": "2b6b6d13-5977-4ba6-9bc3-20da877b3435"
     },
     {
      "id": "7b4ba314-fbea-4e9d-bcf1-736e34edbaac"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "language": null,
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
