{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "7cd75312-2fa5-45e9-ab67-a77d07e90da1",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# 🚀 Exercise 1 - Developing Spark Applications  \n",
                "\n",
                "Welcome to this hands-on lab! In this exercise, you'll explore key concepts and techniques for building efficient Spark applications. Let's dive in!  \n",
                "\n",
                "## 🎯 What You'll Learn \n",
                "\n",
                "By the end of this lab, you'll gain insights into:  \n",
                "- The basics of how Spark works\n",
                "- The _Medallion_ architecture\n",
                "- Development environments: Fabric UI, VS Code Desktop, and VS Code Web\n",
                "- The differences between Markdown and Code Cells\n",
                "- How to read, transform, and write data using Spark DataFrames\n",
                "- Running notebooks in Standard and High Concurrency modes\n",
                "- Spark Environments: manage libraries, choose pools, and explore autoscaling\n",
                "\n",
                "---\n",
                "\n",
                "**Get Ready to Code!**\n",
                "Now that you have an overview, let's get started with hands-on exercises! 🚀\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bfb6e46c",
            "metadata": {},
            "source": [
                "##### **Preparation**  \n",
                "1. Create two Lakehouses (with schema): `silver` and `gold` in your workspace.  \n",
                "   To create Lakehouses with schema, click on **+Lakehouses** in the left sidebar, then select **Add New Lakehouse**. \n",
                "\n",
                "![Create Lakehouse](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/preparation1.jpg?raw=true) \n",
                "\n",
                "![Create Lakehouse](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/preparation2.jpg?raw=true) \n",
                "\n",
                "![Create Lakehouse](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/preparation3.jpg?raw=true) \n",
                "   \n",
                "2. Make **silver** the default Lakehouse for this notebook.  \n",
                "   This can be done by selecting **silver** as the default in the Lakehouse section in the left as shown in the screenshot below:\n",
                "\n",
                "![Setting Default Lakehouse](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/preparation4.jpg?raw=true)\n",
                "\n",
                "**Important:** Ensure that both `silver` and `gold` Lakehouses are created before proceeding with the exercises, as they will be used in the upcoming steps. Make sure the default Lakehouse is in your workspace. \n",
                "\n",
                "3. Here is the sample data uploaded in bronze Lakehouse  \n",
                "[Lab 1 Data](https://github.com/voidfunction/FabCon25SparkWorkshop/tree/main/module-1-developing-spark/Lab1Data/fhirbronze)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "37c80ce6-7341-469d-a2fd-6d89c158b2dd",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## 1.1 Understanding the Medallion Architecture  \n",
                "\n",
                "In this lab, we'll implement the **Medallion Architecture**, a structured approach to organizing data in layers for better performance and reliability:  \n",
                "\n",
                "- **Bronze Layer** : This layer serves as the initial landing zone for data from the source systems, preserving its original format. This zone is typically **append-only** and **immutable**.  \n",
                "\n",
                "- **Silver Layer**: Data is cleaned, standardized, and stored in **OneLake** in **Flattened Delta** format for better querying. \n",
                "\n",
                "- **Gold Layer**: Optimized for analytics, and stored in **OneLake**. It can have denormalized tables or fact and dimension tables :\n",
                "    - Denormalized Tables – Optimized for fast querying in reporting tools like Power BI. These tables combine fact and dimension data to reduce the need for joins.\n",
                "    - Fact and Dimension Tables – A more normalized approach, useful for maintaining data integrity and flexibility while still being optimized for analytics.\n",
                "\n",
                "  The choice depends on your use case:\n",
                "  - Denormalized tables are best for performance and ease of use in reporting.\n",
                "  - Fact and dimension tables provide flexibility and are useful for more complex analytical models. \n",
                "\n",
                "**However, there is no single correct interpretation of the Medallion layers. Organizations decide the architecture based on their specific data needs:\n",
                "- Some organizations introduce more than three layers to support additional transformations or governance requirements.\n",
                "- Some use Bronze for Delta storage, treating Silver as a logical layer with views rather than a separate physical storage layer.\n",
                "Ultimately, the goal is to create value from data while ensuring logical organization of transformations to support replayability, comprehensive auditing, and scalable data consumption at the required level of cleanliness.**\n",
                "\n",
                "![MEDALLION LAYERS](https://learn.microsoft.com/en-us/fabric/onelake/media/onelake-medallion-lakehouse-architecture/onelake-medallion-lakehouse-architecture-example.png)\n",
                "\n",
                "This layered approach ensures data is efficiently processed, transformed, and made ready for analysis. 🚀  \n",
                "\n",
                "---\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "04d2a9f8-b13d-4c60-a21d-a0128d3c69fa",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## 1.2 Notebook Development: Choosing the Right Interface [10 minutes]\n",
                "\n",
                "When developing your Spark applications interactvely, Microsoft Fabric Notebooks offer two flexible options:  \n",
                "- A **web-based interactive interface** (Fabric UI)  \n",
                "- **VS Code integration**\n",
                "\n",
                "Let's explore both! 🚀  "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c60961ad-2187-4891-917a-5e34710521c6",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 1.2.1 Developing in Fabric UI  \n",
                "\n",
                "The **Fabric UI Notebook** is the easiest way to get started—no setup required! If you have **contributor access** to a Fabric workspace, you can create and run notebooks directly in your browser.  \n",
                "\n",
                "#### How to Create a Notebook in Fabric UI  \n",
                "1. Click the **Fabric logo** in the bottom-left corner of the screen.\n",
                "\n",
                "![Fabric UI](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.2.1a.jpg?raw=true)  \n",
                "\n",
                "2. You'll see options for **Fabric** and **Power BI**—select **Fabric**.  \n",
                "\n",
                "![Select Fabric in the Option](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.2.1b.jpg?raw=true) \n",
                "\n",
                "3. Choose your **workspace**.  Click **New Item** → **Notebook** to create a new notebook. \n",
                "\n",
                "![Creating a new Notebook](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.2.1c.jpg?raw=true) \n",
                "\n",
                "Alternatively, if you're importing a pre-built Notebook, click on import.\n",
                "\n",
                "![Importing Notebook](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.1.1a.jpg?raw=true)\n",
                "\n",
                "\n",
                "4. Click next to the **Notebook icon** to rename your notebook.  \n",
                "\n",
                "![Renaming a Notebook](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.2.1d.jpg?raw=true) \n",
                "\n",
                "That's it! You're ready to start coding in Spark in Fabric Notebook ! ✨  \n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "24332f6c-aa34-4a85-b0df-7747475a1d55",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 1.2.2 Integrating with Visual Studio Code (🛠 Presentation, No Hands-On)\n",
                "\n",
                "🎥 **See the Visual Studio integration in action!**  \n",
                "👉 [Click here to watch the video](https://www.youtube.com/watch?v=7TGsTd1SdoU)  \n",
                "\n",
                "Here are the detailed steps:\n",
                "\n",
                "You can integrate with Visual studio desktop or web. For this lab, we will set up Visual Studio web. \n",
                "\n",
                "#### 1. Install the Fabric Data Engineering VS Code extension for the Web\n",
                "\n",
                "Navigate to https://vscode.dev in your browser.\n",
                "\n",
                "![vscode.dev](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.1.1b.jpg?raw=true) \n",
                "\n",
                "Select the Extensions icon in the left navigation bar.\n",
                "\n",
                "![Fabric Engineering Extension](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.1.1c.jpg?raw=true) \n",
                "\n",
                "Search for Fabric Data Engineering and select the Fabric Data Engineering VS Code - Remote pre-release\n",
                "\n",
                "![Fabric Engineering Extension](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.1.1d.jpg?raw=true) \n",
                "\n",
                "click Install.\n",
                "\n",
                "![Installing Fabric Engineering Extension](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.1.1e.jpg?raw=true) \n",
                "\n",
                "#### 2. Open the Notebook with VS Code Web \n",
                "\n",
                "Open your current notebook in the VS Code for the Web experience by clicking the Open in VS Code(Web) button in the notebook authoring page in the Fabric portal.\n",
                "\n",
                "![Open Notebook with VS Code Web](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.1.2b.jpg?raw=true) \n",
                "\n",
                "select Fabric Runtime as the kernel and then select PySpark.\n",
                "\n",
                "If configured correctly, you will see Fabric Runtime in the selected kernel. \n",
                "\n",
                "If you're have a new notebook, you can test execution by adding a simple command such as:\n",
                "\n",
                "~~~python\n",
                "print(5)\n",
                "~~~\n",
                "\n",
                "Then execute the cell. You can update a cell and ctrl + s to save the work. \n",
                "\n",
                "![Selected Kernel](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.1.2f.jpg?raw=true) \n",
                "\n",
                "![Updating Notebook](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.1.2g.jpg?raw=true) \n",
                "\n",
                "***Note**: Let's continue doing our labs in Notebook in Fabric UI.*\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b0eb0f77-e78a-49ec-a95c-e719bd526fce",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 1.2.3 Understanding Markdown vs. Code Cells\n",
                "In Fabric Notebook, you can use Markdown and Code cells to enhance your development and collaboration. \n",
                "\n",
                "1. To insert a new Mardown or Code cell, hover above or below an existing one. You'll see options to add either a Markdown or Code cell—simply select the type you need!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "44661a56-9c50-467e-9763-9716a9b271c5",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "input=5\n",
                "print(input)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f8195bbb-6b7c-46d1-ac98-cbb7de04d0cd",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "**2. Using PySpark, Scala, or Spark SQL**: \n",
                "You can use PySpark, Scala, or Spark SQL for processing data. Choose the language you are most comfortable with. By default, the Notebook will show the PySpark kernel. \n",
                "\n",
                "You can change the kernel at the notebook level or cell level with:\n",
                "\n",
                "- %%spark for Scala.\n",
                "- %%pyspark for PySpark.\n",
                "- %%sql for Spark SQL."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0730afb7-147f-4f55-8552-b17a75806c9c",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## 🔥 Challenge Yourself with PySpark!  \n",
                "\n",
                "Below is a PySpark code snippet that defines a list named `data`, which contains three rows:  \n",
                "\n",
                "- 🟢 **First row:** `id=1`, `name=\"Alice\"`, `age=25`  \n",
                "- 🔵 **Second row:** `id=2`, `name=\"Bob\"`, `age=30`  \n",
                "- 🟠 **Third row:** `id=3`, `name=\"Charlie\"`, `age=35`  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dc225118-a107-4082-b9ef-eea56f5e1637",
            "metadata": {
                "collapsed": false,
                "jupyter": {
                    "outputs_hidden": false
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%pyspark\n",
                "# Import required libraries\n",
                "from pyspark.sql import Row\n",
                "\n",
                "# Create a sample DataFrame with 3 records\n",
                "data = [Row(id=1, name=\"Alice\", age=25),\n",
                "        Row(id=2, name=\"Bob\", age=30),\n",
                "        Row(id=3, name=\"Charlie\", age=35)]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "436c3355-f627-4285-b052-ece755b8a90a",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## 💡 Your Task:\n",
                "Write a PySpark code snippet to:  \n",
                "1️⃣ Create a DataFrame from the data.  \n",
                "2️⃣ Display all the records in the DataFrame.  \n",
                "3️⃣ Print the schema of the DataFrame.  \n",
                "\n",
                "🚀 **Think you got this? Give it a try!**  \n",
                "\n",
                "🔍 **Hint:** You need to use `spark.createDataFrame()` to convert `data` into a DataFrame.  \n",
                "\n",
                "<details>\n",
                "  <summary><strong>🔑 Answer:</strong> Click to reveal</summary>\n",
                "\n",
                "~~~python\n",
                "# Create a DataFrame from the data\n",
                "df = spark.createDataFrame(data)\n",
                "\n",
                "# Display the DataFrame\n",
                "display(df)\n",
                "\n",
                "# Print the Schema of the Dataframe\n",
                "df.printSchema()\n",
                "~~~\n",
                "  \n",
                "</details>\n",
                "\n",
                "<br>\n",
                "\n",
                "> ℹ️ _In PySpark you can also pass in a list of tuples or dictionaries and the schema will be automatically inferred._\n",
                "\n",
                "**Using Python Dictionaries**\n",
                "```python\n",
                "data = [\n",
                "    {\"id\": 1, \"name\": \"Alice\", \"age\": 25},\n",
                "    {\"id\": 2, \"name\": \"Bob\", \"age\": 30},\n",
                "    {\"id\": 3, \"name\": \"Charlie\", \"age\": 35}\n",
                "]\n",
                "\n",
                "df = spark.createDataFrame(data)\n",
                "display(df)\n",
                "```\n",
                "\n",
                "**Using Python Tuples**\n",
                "```python\n",
                "data = [\n",
                "    (1, \"Alice\", 25),\n",
                "    (2, \"Bob\", 30),\n",
                "    (3, \"Charlie\", 35)\n",
                "]\n",
                "\n",
                "columns = [\"id\", \"name\", \"age\"]\n",
                "\n",
                "df = spark.createDataFrame(data, schema=columns)\n",
                "display(df)\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "23b52681-bfc8-409d-9f32-4d2a2cab6bd8",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## 🚀 Scala Spark Challenge!  \n",
                "\n",
                "Here is the equivalent code in **Scala Spark**.  \n",
                "\n",
                "### 📝 **Your Task:**  \n",
                "To run the Scala code, use the `%%spark` magic command like below.  \n",
                "\n",
                "📌 **Challenge:** Modify the code to **pass a static schema** instead of using schema inference.  \n",
                "\n",
                "🔹 **Step 1:** Run the cell below.  \n",
                "🔹 **Step 2:** Copy and Modify the code in a new cell to define a **manual schema** instead of inferring it.  \n",
                "\n",
                "💡 **Hint:** Use `StructType` and `StructField` to define the schema explicitly.  \n",
                "\n",
                "---\n",
                "\n",
                "### ✅ **Run This Cell to Start!** "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "300116d4-7f96-4ec7-9d37-758201b3b3e3",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%spark\n",
                "import org.apache.spark.sql.Row\n",
                "import org.apache.spark.sql.types._\n",
                " \n",
                "val data = Seq(\n",
                "  (1, \"Alice\", 25),\n",
                "  (2, \"Bob\", 30),\n",
                "  (3, \"Charlie\", 35)\n",
                ")\n",
                " \n",
                "val df = spark.createDataFrame(data).toDF(\"id\", \"name\", \"age\")\n",
                " \n",
                "// Show DataFrame\n",
                "display(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cc807d12",
            "metadata": {},
            "source": [
                "**Note**: The toDF() method in Spark is a convenience method that allows you to easily convert a collections (such as a Seq, List, or RDD) into a DataFrame and assign column names."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9f20df43-ddbc-4410-9143-f46c9147296a",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### **Answer:**  \n",
                "Run the cell below\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f22cbe44-3725-40c4-ac99-36084b03ebd8",
            "metadata": {
                "collapsed": false,
                "jupyter": {
                    "outputs_hidden": true,
                    "source_hidden": true
                },
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%spark\n",
                "import org.apache.spark.sql.types._\n",
                "\n",
                "// Define schema\n",
                "val schema = StructType(Seq(\n",
                "  StructField(\"id\", IntegerType, nullable = false),\n",
                "  StructField(\"name\", StringType, nullable = false),\n",
                "  StructField(\"age\", IntegerType, nullable = false)\n",
                "))\n",
                "\n",
                "// Create sample data\n",
                "val data = Seq(\n",
                "  Row(1, \"Alice\", 25),\n",
                "  Row(2, \"Bob\", 30),\n",
                "  Row(3, \"Charlie\", 35)\n",
                ")\n",
                "\n",
                "// Create DataFrame\n",
                "val df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)\n",
                "\n",
                "display(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3144830e-414c-4012-8d7c-bcbd5b455d32",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## 🎯 **Challenge: Spark SQL Edition!**\n",
                "\n",
                "Want to try your hand at Spark SQL? It’s time to switch gears and use SQL to achieve the same goal! 😎\n",
                "\n",
                "### **Here’s what you need to do:**\n",
                "\n",
                "Instead of using PySpark code, you’ll use the `%%sql` magic command to execute SQL queries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bba57f14-2483-4988-81ab-d2c435555a60",
            "metadata": {
                "collapsed": false,
                "jupyter": {
                    "outputs_hidden": false
                },
                "microsoft": {
                    "language": "sparksql",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n",
                "SELECT 1 AS id, 'Alice' AS name, 25 AS age UNION ALL\n",
                "SELECT 2 AS id, 'Bob' AS name, 30 AS age UNION ALL\n",
                "SELECT 3 AS id, 'Charlie' AS name, 35 AS age;"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bf841b88",
            "metadata": {},
            "source": [
                "## 1.3 Spark Basics: Reading, Transforming, and Writing Data with Bronze, Silver, and Gold Layers\n",
                "\n",
                "### 1.3.1 Bronze Layer: Load Raw Data into a **DataFrame (DF)**\n",
                "\n",
                "  We have raw **FHIR Patient** and **Observations** data are stored in **bronze Lakehouse**. We have given you read access to this Lakehouse. \n",
                "  \n",
                "  For this lab, you can acess the raw data **observationsraw** and **patientraw** in **bronze** Lakehouse. \n",
                "\n",
                "  To add **bronze** lakehouse to the Notebook, click +Lakehouses and select the bronze in existing Lakehouses with schema. \n",
                "\n",
                "  ![Attaching Lakehouse](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.3.1a1.jpg?raw=true)\n",
                "\n",
                "  In Spark, you can load data into a DataFrame using the `spark.read()` method. This allows you to read from a variety of source formats such as CSV, JSON, Parquet, Avro, ORC, Delta, and many others. \n",
                "  \n",
                "  You can refer to the Spark documentation for the methods to read different formats: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.html\n",
                "\n",
                "  In this lab, we'll focus on reading **Parquet** files and **streaming JSON** files. We will read the observationsraw (JSON files) and patientraw (Parquet files) from bronze Lakehouse. \n",
                "\n",
                "  ![Files in Lakehouse](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.3.1a2.jpg?raw=true)\n",
                "\n",
                "\n",
                "  #### **Load JSON Data into a DataFrame**\n",
                "\n",
                "  **What is a Dataframe?**\n",
                "\n",
                "  In Spark, a DataFrame is a distributed collection of data organized into named columns similar to an SQL table. It is similar to a table in a relational database or a spreadsheet in that it has a schema, which defines the types and names of its columns, and each row represents a single record or observation."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "49019948",
            "metadata": {},
            "source": [
                "##### Requirements\n",
                "✅ Silver Lakehouse should be the Default Lakehouse"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7fa43466-7ebf-4485-8497-fa2b94889bea",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### Load JSON Data into a DataFrame\n",
                "\n",
                "To begin, let's load a JSON file from bronze Lakehouse files into a Spark DataFrame. \n",
                "\n",
                "In a new code cell, use the following code to:\n",
                "- read the JSON files\n",
                "- print the schema\n",
                "- display the data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8bb0bcf3-5126-42f7-9e1a-cc7161f3fec3",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import from_json,col\n",
                "\n",
                "observations_path=\"abfss://47938747-73b4-4f78-99dc-4ff2afa78142@onelake.dfs.fabric.microsoft.com/443d992d-01f1-4caa-9345-088e81dd81df/Files/observationsraw\"\n",
                "# Load JSON data\n",
                "observations_raw_df = spark.read.json(observations_path)\n",
                "\n",
                "observations_raw_df.printSchema()\n",
                "\n",
                "display(observations_raw_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fd2d0eb3-d76f-448c-a0b6-b6aceba2178b",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### Read Parquet Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fbe4de9d-9256-4a6e-a5de-602a67eb9520",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "patient_path = \"abfss://47938747-73b4-4f78-99dc-4ff2afa78142@onelake.dfs.fabric.microsoft.com/443d992d-01f1-4caa-9345-088e81dd81df/Files/patientraw\"\n",
                "\n",
                "# Read Parquet files into a DataFrame\n",
                "patient_raw_df = spark.read.parquet(patient_path)\n",
                "\n",
                "patient_raw_df.printSchema()\n",
                "\n",
                "# Show the contents of the DataFrame\n",
                "display(patient_raw_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b207dac7-1bef-4a41-a703-495ffac71cbd",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 1.3.2 Silver Layer: Cleaning, De-duplicating, and Flattening Data\n",
                "\n",
                "Now that we've loaded both JSON and Parquet data into DataFrames, let's clean the data and flatten nested structures like arrays in the Observations and Patient datasets to prepare them for the Silver Layer.\n",
                "\n",
                "To enable SQL queries on Spark DataFrames, we will create temporary views using .createOrReplaceTempView()."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "14b05cf9-3a2a-480c-9790-a3d2cf7d2b97",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "observations_raw_df.createOrReplaceTempView(\"observations_raw_view\")\n",
                "patient_raw_df.createOrReplaceTempView(\"patient_raw_view\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "79f73053",
            "metadata": {},
            "source": [
                "#### Understanding explode() and Accessing Structs in Spark\n",
                "In Spark, the explode() function is designed specifically to work with arrays. If you have a column that is a struct, you cannot use explode() directly. Instead, you can access its fields by using dot notation.\n",
                "\n",
                "For example, column encounter is a struct, you can access its field reference like this:\n",
                "\n",
                "~~~sql\n",
                "SELECT encounter.reference FROM observations_raw_view;\n",
                "~~~\n",
                "\n",
                "To flatten an array, you can use the explode() function. \n",
                "\n",
                "But what if you have an array of structs? To flatten and access an array of structs, you can use the LATERAL VIEW EXPLODE() combination.\n",
                "\n",
                "For example, to explode the category array and access its coding field, use:\n",
                "\n",
                "~~~sql\n",
                "SELECT category_exploded.coding\n",
                "FROM observations_raw_view\n",
                "LATERAL VIEW EXPLODE(category) AS category_exploded;\n",
                "~~~\n",
                "This method allows you to break down arrays of complex types, like structs, into individual rows—making your data easier to work with."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "91fd9045",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "sparksql",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n",
                "\n",
                "SELECT encounter.reference FROM observations_raw_view;"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "67b8c669",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "sparksql",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n",
                "\n",
                "SELECT category_exploded.coding FROM observations_raw_view\n",
                "LATERAL VIEW EXPLODE(category) AS category_exploded;"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "35636bc8-83aa-4480-9b20-622a75aa32df",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### Flattening and Selecting Key Columns in the Silver Layer\n",
                "\n",
                "In the Silver Layer, we typically flatten the tables from the Bronze Layer while retaining important columns. Flattening helps normalize nested structures like arrays and structs, making the data easier to query and process.\n",
                "\n",
                "However, for simplicity in this lab, we will write an SQL query to extract only a few key columns from the `observations_raw_view`, rather than keeping all columns from the Bronze Layer."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "609bf014-e4be-4199-8b4c-1f8e33a450f8",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "In `observations_raw_view`, since `category` is an array, we use `LATERAL VIEW OUTER EXPLODE(category)` to extract its elements, keeping null values if the array is empty.\n",
                "\n",
                "Below is the SQL query to flatten and extract key fields from the `observations_raw_view`:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1b9c353d-4f2f-4107-8e46-ece3d6475aba",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "sparksql",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n",
                "SELECT\n",
                "    id,\n",
                "    resourceType,\n",
                "    status,\n",
                "    subject.reference AS subject_reference,\n",
                "    SPLIT(subject.reference, '/')[1] AS patient_id,\n",
                "    encounter.reference AS encounter_reference,\n",
                "    \n",
                "    -- Extract category code and display (first element of array)\n",
                "    category_array.coding[0].code AS category_code,\n",
                "    category_array.coding[0].display AS category_display,\n",
                "\n",
                "    -- Extract observation code details\n",
                "    code.coding[0].code AS observation_code,\n",
                "\n",
                "    -- Extract effective date\n",
                "    effectiveDateTime,\n",
                "\n",
                "    -- Extract value details (Quantity, CodeableConcept, String)\n",
                "    valueQuantity.value AS value_quantity,\n",
                "    valueQuantity.unit AS value_unit\n",
                "\n",
                "FROM observations_raw_view\n",
                "LATERAL VIEW OUTER EXPLODE(category) category_table AS category_array"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "49f0c40e-b2e4-4881-93e1-3b18c63ab228",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "Next, we will write a SQL query to extract key columns from the patient_raw_view and flatten nested array fields using EXPLODE."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "89219f1d-6cb1-48f6-a059-3f908b397951",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "sparksql",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n",
                "SELECT \n",
                "    p.id,\n",
                "    p.gender,\n",
                "    p.birthDate,\n",
                "    p.deceasedDateTime,\n",
                "\n",
                "    -- Flatten name details\n",
                "    name_array.family AS last_name,\n",
                "    name_array.given[0] AS first_name  -- Extract first element from array\n",
                "\n",
                "FROM patient_raw_view p\n",
                "LATERAL VIEW OUTER EXPLODE(p.name) name_table AS name_array"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "87c05d87-6ba2-408d-95c7-8658013ea750",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### Create and Insert to Flattened Observations Table in the Silver Layer\n",
                "\n",
                "Next, we will create and insert to a new observations_silver table in the Silver layer with a flattened schema. \n",
                "\n",
                "This table will be stored in the silvercleaned lakehouse. \n",
                "\n",
                "Specify the table location as silvercleaned.dbo.observations_silver."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e7f6e195-63d5-4439-8bef-bee1bbb28c40",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "sparksql",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n",
                "CREATE OR REPLACE TABLE silver.dbo.observations\n",
                "AS \n",
                "SELECT\n",
                "    id,\n",
                "    resourceType,\n",
                "    status,\n",
                "    subject.reference AS subject_reference,\n",
                "    SPLIT(subject.reference, '/')[1] AS patient_id,\n",
                "    encounter.reference AS encounter_reference,\n",
                "    \n",
                "    -- Extract category code and display (first element of array)\n",
                "    category_struct.coding[0].code AS category_code,\n",
                "    category_struct.coding[0].display AS category_display,\n",
                "\n",
                "    -- Extract observation code details\n",
                "    code.coding[0].code AS observation_code,\n",
                "\n",
                "    -- Extract effective date\n",
                "    effectiveDateTime,\n",
                "\n",
                "    -- Extract value details (Quantity, CodeableConcept, String)\n",
                "    valueQuantity.value AS value_quantity,\n",
                "    valueQuantity.unit AS value_unit\n",
                "\n",
                "FROM observations_raw_view\n",
                "LATERAL VIEW OUTER EXPLODE(category) category_table AS category_struct"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "13cc548e-253d-49a7-a515-7487bdef922e",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### Creating the Patient Silver Table in the Silvercleaned Lakehouse\n",
                "\n",
                "The following SQL code creates the `patient_silver` table in the `silvercleaned` lakehouse if it doesn't already exist:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3806193c-6b32-49f2-8ce6-9aeace492d1c",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "sparksql",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n",
                "CREATE OR REPLACE TABLE silver.dbo.patient (\n",
                "    id STRING,\n",
                "    gender STRING,\n",
                "    birthDate DATE,\n",
                "    deceasedDateTime TIMESTAMP,\n",
                "    last_name STRING,\n",
                "    first_name STRING\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c98b3268-2550-480e-b04d-5ddba6d12806",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "Writing a SQL code to flatten nested types and insert to the `patient_silver` table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6ed206ab-0f5c-4ba0-be85-3748d27fa878",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "sparksql",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n",
                "INSERT INTO silver.dbo.patient\n",
                "\n",
                "SELECT \n",
                "    p.id,\n",
                "    p.gender,\n",
                "    p.birthDate,\n",
                "    p.deceasedDateTime,\n",
                "\n",
                "    -- Flatten name details\n",
                "    name_struct.family AS last_name,\n",
                "    name_struct.given[0] AS first_name  -- Extract first element from array\n",
                "\n",
                "FROM patient_raw_view p\n",
                "LATERAL VIEW OUTER EXPLODE(p.name) name_table AS name_struct"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8e066b4d-9349-4842-9c91-208555c6552f",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 1.3.3 Gold Layer: Creating a Denormalized Table\n",
                "\n",
                "We will perform an **INNER JOIN** between the `Patient` and `Observations` tables to create a denormalized table in the Gold Layer. This table will be used for reporting and analytics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6e529687-eaa0-40ce-af0a-f943249b170f",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "sparksql",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n",
                "-- Creating the Gold Layer with necessary patient and observation details\n",
                "CREATE OR REPLACE TABLE gold.dbo.patientobservations\n",
                "AS\n",
                "SELECT\n",
                "    o.id AS observation_id,\n",
                "    o.resourceType,\n",
                "    o.status,\n",
                "    o.patient_id,\n",
                "    o.encounter_reference,\n",
                "    \n",
                "    -- Patient details from the patient_silver table\n",
                "    p.gender,\n",
                "    p.birthDate,\n",
                "    p.deceasedDateTime,\n",
                "    p.last_name,\n",
                "    p.first_name,\n",
                "\n",
                "    -- Observation specific details\n",
                "    o.category_code,\n",
                "    o.category_display,\n",
                "    o.observation_code,\n",
                "    o.effectiveDateTime,\n",
                "    o.value_quantity,\n",
                "    o.value_unit\n",
                "\n",
                "FROM silver.dbo.observations o\n",
                "JOIN silver.dbo.patient p\n",
                "    ON o.patient_id = p.id"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4e3ab0ff-d131-4a8e-abd6-62b5012fc02e",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "##### Select the Top 10 Patients with the Most Observations\n",
                "\n",
                "Next, let's query the top 10 patients who have the highest number of observations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ca2a6e4b-774a-4778-83e4-974061cffe86",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "sparksql",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n",
                "SELECT\n",
                "    patient_id,\n",
                "    gender,\n",
                "    COUNT(observation_id) AS number_of_observations\n",
                "FROM gold.dbo.patientobservations\n",
                "GROUP BY\n",
                "    patient_id, gender\n",
                "ORDER BY\n",
                "    number_of_observations DESC\n",
                "LIMIT 10"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c0ec35dc",
            "metadata": {},
            "source": [
                "### 1.5 Configuring & Publishing Your Spark Environment (🛠 Presentation, No Hands-On)\n",
                "\n",
                "🎥 **Watch Configuring Custom Pool and Environment in Action**  \n",
                "👉 [Click here to watch the video](https://www.youtube.com/watch?v=4eaT4zzDxgU)\n",
                "\n",
                "#### 🚀 Set Up Your Spark Environment with Ease!  \n",
                "\n",
                "In this lab, you'll explore how to configure and publish your Spark environment efficiently. By the end, you’ll know how to:  \n",
                "\n",
                "✅ Manage **libraries** and dependencies  \n",
                "✅ Choose between **Starter Pools vs Custom Pools**  \n",
                "✅ Leverage **Autoscaling & Dynamic Allocation** for optimal performance  \n",
                "\n",
                "---\n",
                "\n",
                "### 🔧 Step 1: Select or Create a Spark Environment  \n",
                "\n",
                "To get started, open the **Notebook** and click on the **Environment** dropdown, as shown below:  \n",
                "\n",
                "![Selecting the environment](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.5.1a1.jpg?raw=true)  \n",
                "\n",
                "If your Spark job requires additional libraries, you can **add public or custom libraries**:  \n",
                "\n",
                "![Adding custom libraries](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.5.1a.jpg?raw=true)  \n",
                "\n",
                "Once you've made your changes, remember to **Save & Publish** for them to take effect.  \n",
                "\n",
                "---\n",
                "\n",
                "### ⚡ Step 2: Choose the Right Compute Pool  \n",
                "\n",
                "In **Fabric Spark**, you can choose between:  \n",
                "\n",
                "- **Starter Pools** (Pre-provisioned, always-on clusters for quick execution)  \n",
                "- **Custom Pools** (User-defined clusters with flexible scaling options)  \n",
                "\n",
                "#### ✨ Starter Pools  \n",
                "Starter pools allow you to run Spark within **seconds** without waiting for nodes to set up. These clusters are **always available**, dynamically scaling up based on job demands.  \n",
                "\n",
                "#### 🛠️ Custom Spark Pools  \n",
                "If you need more control, **Custom Pools** let you define the number and size of nodes \n",
                "\n",
                "To customize or create a Spark pool, go to **Workspace Spark Settings** (admin access required). For this lab, we’ve already set up a **custom pool**.  \n",
                "\n",
                "📸 Screenshots for reference:  \n",
                "\n",
                "![Compute Selection](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.5.2a.jpg?raw=true)  \n",
                "\n",
                "![Pool Configuration](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.5.2b.jpg?raw=true)  \n",
                "\n",
                "![Finalize Settings](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.5.2c.jpg?raw=true)  \n",
                "\n",
                "#### Optimize Application with Autoscaling & Dynamic Allocation  \n",
                "\n",
                "To ensure efficient resource usage, **Autoscaling** and **Dynamic Allocation** help manage compute resources dynamically.  \n",
                "\n",
                "#### 🔄 Autoscaling  \n",
                "- Automatically **scales up or down** based on **YARN pending resources** (Memory & CPU).  \n",
                "- Adjusts cluster size based on workload demand.  \n",
                "\n",
                "#### ⚙️ Dynamic Allocation  \n",
                "- Dynamically **adds or removes executors** based on  tasks backlog and executor idle time.  \n",
                "- Avoids over-provisioning and reduces idle resources.  \n",
                "\n",
                "    💡 **How It Works:**  \n",
                "    - You define **minimum and maximum nodes** for autoscaling.  \n",
                "    - Spark automatically adjusts nodes based on demand.  \n",
                "    - The system dynamically assigns executors **only when needed**, ensuring efficiency.  \n",
                "\n",
                "📸 Selecting Compute Pools:  \n",
                "\n",
                "![Pool Selection](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/screenshots/module-1-developing-spark/1.5.3a.jpg?raw=true)  \n",
                "\n",
                "---\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3389af19-eeb7-4cb0-be55-684c6d1245d2",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## 🌟 Bonus - Spark Structured Streaming"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f4070594",
            "metadata": {},
            "source": [
                "#### Read Streaming JSON Data\n",
                "In this section, we will walk through how to read streaming JSON data from Lakehouse Files using Apache Spark's structured streaming capabilities. This method allows Spark to continuously read new JSON files as they arrive, while ignoring the files that have already been processed. It’s especially useful for processing both historical and real-time data streams.\n",
                "\n",
                "1. **Define the Path to Your Data**: Start by specifying the ABFS path to the JSON files in bronze Lakehouse.\n",
                "\n",
                "2. **Set Up the Spark ReadStream**: With the file path defined, use `spark.readStream` to initiate the streaming read operation. The `.json()` method reads the incoming JSON data from the specified path.\n",
                "\n",
                "    ~~~python\n",
                "    input_df = spark.readStream.json(shortcut_path)\n",
                "    ~~~\n",
                "\n",
                "3. **Defining the Schema**: When working with streaming data, it's crucial to define a schema to understand the structure of the incoming data. In this case, we define the schema as follows:\n",
                "\n",
                "    ~~~python\n",
                "    observations_schema = StructType([\n",
                "        StructField(\"id\", StringType(), True),\n",
                "        StructField(\"resourceType\", StringType(), True),\n",
                "        StructField(\"status\", StringType(), True),\n",
                "        StructField(\"subject\", StructType([  # Nested structure\n",
                "            StructField(\"reference\", StringType(), True)\n",
                "        ]), True),\n",
                "        StructField(\"category\", StructType([  # Nested structure\n",
                "            StructField(\"coding\", StructType([  # Nested structure\n",
                "                StructField(\"code\", StringType(), True),\n",
                "                StructField(\"display\", StringType(), True)\n",
                "            ]), True)\n",
                "        ]), True),\n",
                "        StructField(\"code\", StructType([  # Nested structure\n",
                "            StructField(\"coding\", StructType([  # Nested structure\n",
                "                StructField(\"code\", StringType(), True),\n",
                "                StructField(\"display\", StringType(), True)\n",
                "            ]), True)\n",
                "        ]), True),\n",
                "        StructField(\"effectiveDateTime\", StringType(), True),\n",
                "        StructField(\"valueQuantity\", StructType([  # Nested structure\n",
                "            StructField(\"value\", DoubleType(), True),\n",
                "            StructField(\"unit\", StringType(), True)\n",
                "        ]), True)\n",
                "    ])\n",
                "    ~~~\n",
                "\n",
                "4. **Streaming with `availableNow` for Historical Data**: If you're processing historical data, you can use the `trigger(availableNow=True)` option to process all available data in one go. This will handle data that has already arrived, ensuring it’s processed immediately.\n",
                "\n",
                "    ~~~python\n",
                "    query = parsed_observations_df.writeStream \\\n",
                "        .outputMode(\"append\") \\\n",
                "        .format(\"delta\") \\\n",
                "        .trigger(availableNow=True) \\\n",
                "        .option(\"checkpointLocation\", \"<check_point_location>\") \\\n",
                "        .toTable(\"observations_streaming_silver\")  # Save the stream to the Delta table\n",
                "    ~~~"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "034b2b34-f1cb-4432-b713-f4ec3c91b9ff",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import from_json, col\n",
                "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
                "\n",
                "# Define schema for the expected JSON structure (based on observations data)\n",
                "observations_schema = StructType([\n",
                "    StructField(\"id\", StringType(), True),\n",
                "    StructField(\"resourceType\", StringType(), True),\n",
                "    StructField(\"status\", StringType(), True),\n",
                "    StructField(\"subject\", StructType([  # Nested structure\n",
                "        StructField(\"reference\", StringType(), True)\n",
                "    ]), True),\n",
                "    StructField(\"category\", StructType([  # Nested structure\n",
                "        StructField(\"coding\", StructType([  # Nested structure\n",
                "            StructField(\"code\", StringType(), True),\n",
                "            StructField(\"display\", StringType(), True)\n",
                "        ]), True)\n",
                "    ]), True),\n",
                "    StructField(\"code\", StructType([  # Nested structure\n",
                "        StructField(\"coding\", StructType([  # Nested structure\n",
                "            StructField(\"code\", StringType(), True),\n",
                "            StructField(\"display\", StringType(), True)\n",
                "        ]), True)\n",
                "    ]), True),\n",
                "    StructField(\"effectiveDateTime\", StringType(), True),\n",
                "    StructField(\"valueQuantity\", StructType([  # Nested structure\n",
                "        StructField(\"value\", DoubleType(), True),\n",
                "        StructField(\"unit\", StringType(), True)\n",
                "    ]), True)\n",
                "])\n",
                "\n",
                "# Path to the observations JSON data (adjust as needed)\n",
                "observations_shortcut_path = \"abfss://47938747-73b4-4f78-99dc-4ff2afa78142@onelake.dfs.fabric.microsoft.com/443d992d-01f1-4caa-9345-088e81dd81df/Files/observationsraw\"\n",
                "\n",
                "# Read JSON data into a streaming DataFrame with schema specified\n",
                "observations_raw_df = spark.readStream.schema(observations_schema).json(observations_path)\n",
                "\n",
                "# Print schema to confirm the structure\n",
                "observations_raw_df.printSchema()\n",
                "\n",
                "# Parse the raw data and select required fields\n",
                "parsed_observations_df = observations_raw_df.select(\n",
                "    \"id\", \"resourceType\", \"status\", \"subject.reference\", \n",
                "    \"category.coding.code\", \"category.coding.display\", \n",
                "    \"effectiveDateTime\", \"valueQuantity.value\", \"valueQuantity.unit\"\n",
                ")\n",
                "\n",
                "# Print the schema of parsed DataFrame\n",
                "parsed_observations_df.printSchema()\n",
                "\n",
                "# Write the micro-batch to a Delta table\n",
                "query = parsed_observations_df.writeStream \\\n",
                "    .outputMode(\"append\") \\\n",
                "    .format(\"delta\") \\\n",
                "    .trigger(availableNow=True) \\\n",
                "    .option(\"checkpointLocation\", f\"abfss://{notebookutils.runtime.context['currentWorkspaceName']}@{spark.conf.get('fs.defaultFS').split('@')[1]}silver.Lakehouse/Files/checkpoints/observations/\") \\\n",
                "    .toTable(\"silver.dbo.observations_streaming\")  # Save the stream to the Delta table\n",
                "\n",
                "# Await termination (to keep the stream running until all available data is processed)\n",
                "query.awaitTermination()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "66e0304e",
            "metadata": {},
            "source": [
                "Now that the batch streaming job has complete, run a `SELECT *` OR `DESCRIBE HISTORY` to verify that data was written."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "97cf5358-4edf-497e-b335-d0db0d7cd7c1",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "sparksql",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n",
                "select * from silver.dbo.observations_streaming"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "080987d8",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "history = spark.sql(\"DESCRIBE HISTORY silver.dbo.observations_streaming\")\n",
                "display(history)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "48b63b59",
            "metadata": {},
            "source": [
                "5. **Switching to Real-Time Data with a 5-second Trigger**: After processing historical data, you can switch to real-time data streaming by using `trigger(processingTime='5 seconds')`. This ensures that new data arriving every 5 seconds is processed immediately, enabling near real-time streaming.\n",
                "\n",
                "    ~~~python\n",
                "    query = parsed_observations_df.writeStream \\\n",
                "        .outputMode(\"append\") \\\n",
                "        .format(\"delta\") \\\n",
                "        .trigger(processingTime='5 seconds') \\\n",
                "        .option(\"checkpointLocation\", \"<check_point_location>\") \\\n",
                "        .toTable(\"observations_streaming_silver\")  # Save the stream to the Delta table\n",
                "    ~~~\n",
                "\n",
                "6. **Write the Data to Streaming Target**: Use .toTable to stream insert to target delta table. Ensure checkpointing is configured for fault tolerance.\n",
                "\n",
                "    ~~~python\n",
                "    query = parsed_observations_df.writeStream \\\n",
                "        .outputMode(\"append\") \\\n",
                "        .format(\"delta\") \\\n",
                "        .trigger(processingTime='5 seconds') \\\n",
                "        .option(\"checkpointLocation\", \"<check_point_location>\") \\\n",
                "        .toTable(\"observations_streaming_silver\")  # Save the stream to the Delta table\n",
                "    ~~~\n",
                "\n",
                "By using `trigger(availableNow=True)` for historical data and `trigger(processingTime='5 seconds')` for real-time data, you can efficiently process both backlogged data and streaming data with Apache Spark.\n",
                "\n",
                "**Tip**: Don't forget to stop the stream once you're done testing, as it will continue running indefinitely."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "40f3ef2f",
            "metadata": {},
            "source": [
                "**If you'd like to test this section, please inform the instructors so they can start the streaming data job in the backend.**\n",
                "\n",
                "Now, run the same streaming job again, but with a 5 second continuous trigger. This will run continuously to process new arriving data every 5 seconds until the cell or Spark Session is terminated. Go ahead and run this and cancel the running cell after 20-30 seconds."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6d078a7e-b769-49dd-8a59-643599ab8b21",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "query = parsed_observations_df.writeStream \\\n",
                "    .outputMode(\"append\") \\\n",
                "    .format(\"delta\") \\\n",
                "    .trigger(processingTime='5 seconds') \\\n",
                "    .option(\"checkpointLocation\", f\"abfss://{notebookutils.runtime.context['currentWorkspaceName']}@{spark.conf.get('fs.defaultFS').split('@')[1]}silver.Lakehouse/Files/checkpoints/observations/\") \\\n",
                "    .toTable(\"silver.dbo.observations_streaming\")  # Save the stream to the Delta table\n",
                "\n",
                "# Streaming job will process new data every 5 seconds continuously until cancelled. \n",
                "query.awaitTermination()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "97c3a6b2",
            "metadata": {},
            "source": [
                "Now query the table transaction log to see how many records (`numOutputRows`) were added in the last streaming write."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5a3d6590",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "history = spark.sql(\"DESCRIBE HISTORY silver.dbo.observations_streaming\")\n",
                "history.collect()[0][\"operationMetrics\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "81d1ffe9",
            "metadata": {},
            "source": [
                "## 🎉 Wrapping Up the Exercise: Developing Spark Applications\n",
                "\n",
                "Congrats on completing this hands-on exercise! 🚀 You've learned the following this chapter:\n",
                "\n",
                "- **Notebook Development**: You’ve explored how to create and work with Spark notebooks in different interfaces like Fabric UI and VS Code, using standard and high concurrency session.\n",
                "\n",
                "- **Spark Basics**: You’ve loaded, transformed, and written data in various formats using DataFrames, and you’ve worked with different layers of data processing to prepare it for analytics.\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "a365ComputeOptions": null,
        "dependencies": {
            "environment": {},
            "lakehouse": {}
        },
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "kernelspec": {
            "display_name": "Synapse PySpark",
            "language": "Python",
            "name": "synapse_pyspark"
        },
        "language_info": {
            "name": "python"
        },
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "sessionKeepAliveTimeout": 0,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1800000"
                }
            }
        },
        "synapse_widget": {
            "state": {},
            "version": "0.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
