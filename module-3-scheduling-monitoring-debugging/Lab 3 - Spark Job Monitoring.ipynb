{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "cfae0421",
            "metadata": {},
            "source": [
                "\n",
                "> [!NOTE]\n",
                "> Timebox: 60 minutes (20 minutes of content | 40 minutes of lab work)\n",
                "> \n",
                "> [Back to Agenda](./../README.md#agenda) | [Back to Start Steps](../module-0-setup/start.md) | [Up next Exercise 2](./../exercise-2/exercise-2.md)\n",
                "\n",
                "# üöÄ Lab 3: Job Scheduling, Monitoring, and Debugging  \n",
                "\n",
                "## üéØ What You'll Learn \n",
                "\n",
                "By the end of this lab, you'll gain insights into:  \n",
                "- **Exploring the Spark UI**: Detect task skews & utilization issues\n",
                "- **Spark Monitoring UI**: Analyze resource usage, view snapshots of your pipeline \n",
                "- Live Application Debugging\n",
                "- High Concurrency (HC) Monitoring\n",
                "- Emitting Spark Logs & Metrics to Event Hub and Blob Storage\n",
                "\n",
                "### üñ•Ô∏è Context: Microsoft Fabric Spark Monitoring  \n",
                "Microsoft Fabric‚Äôs Spark monitoring offers a **web-UI-driven experience** with powerful built-in tools to:  \n",
                "‚úîÔ∏è Track Spark applications in progress.  \n",
                "‚úîÔ∏è Browse historical Spark activity.  \n",
                "‚úîÔ∏è Analyze performance bottlenecks.  \n",
                "‚úîÔ∏è Troubleshoot failures effectively.  \n",
                "\n",
                "With multiple entry points, Spark monitoring ensures seamless access to job details, making it easier to optimize execution and resolve issues.  \n",
                "\n",
                "\n",
                "### Preparation  \n",
                "\n",
                "To get started, follow these steps:  \n",
                "\n",
                "1. **Upload the dataset**  \n",
                "   - Add the file [`online_retail.csv`](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-3-scheduling-monitoring-debugging/online_retail.csv) to your lakehouse.  \n",
                "\n",
                "2. **Import the notebook**  \n",
                "   - Load the [`SparkMonitoring-2025.ipynb`](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-3-scheduling-monitoring-debugging/SparkMonitoring-2025.ipynb) notebook into your environment.  \n",
                "\n",
                "Once completed, you're ready to proceed with this lab ! üöÄ "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "76ebcc79",
            "metadata": {},
            "source": [
                "### 3.1 Monitor Hub  \n",
                "\n",
                "The **Monitor Hub** is the centralized portal for tracking Spark activities across different components. With a quick glance, you can:  \n",
                "\n",
                "- View in-progress Spark applications triggered from **Notebooks, Spark Job Definitions, and Pipelines**.  \n",
                "- Search and filter Spark applications  \n",
                "- Drill down into Spark execution details for deeper insights.  \n",
                "\n",
                "#### Accessing the Monitor Hub  \n",
                "\n",
                "To open the Monitor Hub from the **Fabric portal**, follow these steps:  \n",
                "\n",
                "1. Navigate to the **left sidebar**.  \n",
                "2. Click on **Monitor** to access the monitoring pane.  \n",
                "\n",
                "Here, you can explore your Spark jobs effortlessly.  \n",
                "\n",
                "![Monitor Hub Navigation](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-3-scheduling-monitoring-debugging/_media/monitoring-hub-in-the-left-side-navigation-bar.png?raw=true)  \n",
                "\n",
                "Stay on top of your Spark applications with ease! üöÄ  "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "349a6d9a-4b24-4d3b-a4bb-9a74d20891d3",
            "metadata": {
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 3.2 Item Recent Runs  \n",
                "\n",
                "The **Recent Runs** feature helps you track your current and past activities for specific items. It provides key insights, including:  \n",
                "\n",
                "- **Submitter** ‚Äì Who initiated the job  \n",
                "- **Status** ‚Äì Whether the job is running, completed, or failed  \n",
                "- **Duration** ‚Äì How long the job took  \n",
                "- **Other details** ‚Äì Additional execution metadata  \n",
                "\n",
                "#### Accessing Recent Runs  \n",
                "\n",
                "To view recent runs, follow these steps:  \n",
                "\n",
                "1. Open the **Microsoft Fabric** homepage.  \n",
                "2. Select the **workspace** where your job is running.  \n",
                "3. Locate the relevant **Spark Job Definition, Notebook, or Pipeline**.  \n",
                "4. Open the **context menu** for the item and select **Recent Runs**.  \n",
                "5. The **Recent Runs** pane will display past execution details for easy monitoring.  \n",
                "\n",
                "![Recent Runs List](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-3-scheduling-monitoring-debugging/_media/recent-runs-list.png?raw=true)  "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "23cd4c2a-a9d9-45b4-bb61-6bc8af2d52e2",
            "metadata": {
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 3.3 Notebook Contextual Monitoring  \n",
                "\n",
                "**Notebook Contextual Monitoring** brings everything you need into one place‚Äî**authoring, monitoring, and debugging Spark jobs** seamlessly. With this feature, you can:  \n",
                "\n",
                "- **Track Spark job progress** in real time.  \n",
                "- **View execution details** like tasks and executors.  \n",
                "- **Access Spark logs** directly at the **Notebook cell level**.  \n",
                "- Get **real-time insights** from the built-in **Spark Advisor**, which provides code recommendations, execution analysis, and error debugging.  \n",
                "\n",
                "---\n",
                "\n",
                "#### 3.3.1 Monitor Spark Job Progress  \n",
                "\n",
                "A **real-time progress bar** provides instant visibility into Spark job execution within each Notebook cell. This helps you:  \n",
                "\n",
                "‚úÖ Monitor the status of each Spark job.  \n",
                "‚úÖ Track task execution across different stages.  \n",
                "‚úÖ Troubleshoot efficiently with live updates.  \n",
                "\n",
                "##### Try it out!  \n",
                "\n",
                "üöÄ **Run the cell below** and expand the **Spark Jobs** section to view real-time status updates and task progress across execution stages.  \n",
                "\n",
                "![Spark Monitoring Progress](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-3-scheduling-monitoring-debugging/_media/spark-monitor-progress.png?raw=true)  \n",
                "\n",
                "Stay on top of your Spark jobs like a pro! üí°üî•  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "df592a2b-3491-4705-804e-7daa055b5bdf",
            "metadata": {
                "collapsed": false,
                "jupyter": {
                    "outputs_hidden": false
                },
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-03-25T14:33:20.1154437Z",
                            "execution_start_time": "2025-03-25T14:33:16.7738309Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "0ac02fd0-d886-4b78-829f-3d0b6d056695",
                            "queued_time": "2025-03-25T14:33:06.1970265Z",
                            "session_id": "c8f3477c-cd91-404a-91e9-c547f8e1e160",
                            "session_start_time": "2025-03-25T14:33:06.198027Z",
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 3,
                            "statement_ids": [
                                3
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, c8f3477c-cd91-404a-91e9-c547f8e1e160, 3, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.synapse.widget-view+json": {
                            "widget_id": "3aa98641-d25e-474f-9345-e592e0569065",
                            "widget_type": "Synapse.DataFrame"
                        },
                        "text/plain": [
                            "SynapseWidget(Synapse.DataFrame, 3aa98641-d25e-474f-9345-e592e0569065)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "df: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
                    ]
                }
            ],
            "source": [
                "val df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/online_retail.csv\")\n",
                "display(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "93bba639-118d-484e-9354-cbae24bfc566",
            "metadata": {
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### 3.3.2 Monitor Resource Usage  \n",
                "\n",
                "The **Executor Usage Graph** provides a **visual representation** of **Spark job executors and resource consumption**.  \n",
                "\n",
                "- Currently, only the runtime information of spark 3.4 and above will display this feature. \n",
                "- Displays **real-time resource usage** per code cell.  \n",
                "- Helps optimize performance by analyzing **executor allocation**.  \n",
                "\n",
                "üîπ **Try it out:** Click on the **Resources** tab to view the line chart for your code cell‚Äôs resource usage.  \n",
                "\n",
                "![Resource Usage Graph](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-3-scheduling-monitoring-debugging/_media/resource.png?raw=true)  \n",
                "\n",
                "With **Notebook Contextual Monitoring**, you can **debug faster, optimize better, and gain deeper insights** into your Spark jobs‚Äîall in one place! üöÄ  "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "406d3ad2-587d-499e-bfa6-cf18be60391f",
            "metadata": {
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### 3.3.3 Hands-on Exercise: Monitoring Resource Utilization  \n",
                "\n",
                "##### **Step 1: First, run the following preparation code:**    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "5899948a-b958-4307-9339-123c72795858",
            "metadata": {
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-03-25T15:00:29.5017183Z",
                            "execution_start_time": "2025-03-25T15:00:27.9993476Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "3b8e895c-e635-4cb2-be67-f28bf057ee58",
                            "queued_time": "2025-03-25T15:00:27.9983019Z",
                            "session_id": "c8f3477c-cd91-404a-91e9-c547f8e1e160",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 5,
                            "statement_ids": [
                                5
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, c8f3477c-cd91-404a-91e9-c547f8e1e160, 5, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "import java.util.concurrent.TimeUnit\n",
                        "import scala.util.Random\n",
                        "data: Array[Int] = Array(20, 14, 6, 17, 11, 9, 24, 30, 2, 19, 21, 19, 25, 22, 8, 30, 2, 30, 22, 19, 23, 19, 31, 13, 7, 25, 4, 5, 20, 12, 2, 28)\n",
                        "calculation: (iter: Iterator[Int])Iterator[Int]\n"
                    ]
                }
            ],
            "source": [
                "// Create some fake data and task definition for the demostration below.\n",
                "import java.util.concurrent.TimeUnit\n",
                "import scala.util.Random\n",
                "\n",
                "val data = Array.fill(32)(Random.nextInt(32))\n",
                "\n",
                "def calculation(iter: Iterator[Int]): Iterator[Int] = {\n",
                "    val sum = iter.map(_ => 1).sum\n",
                "    TimeUnit.SECONDS.sleep(10*sum)\n",
                "    Iterator(sum)\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6dbb4cf8-e2d6-437c-8162-d6cf98f7978a",
            "metadata": {
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "##### **Step 2: Run the cell below.**\n",
                "Now, observe the Resource Usage tab.\n",
                "Notice that the job does not utilize all available cores, meaning we should increase parallelism."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "67756448-6e90-4c3f-ae0a-0e9b213a0b8a",
            "metadata": {
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-03-25T15:01:47.3929843Z",
                            "execution_start_time": "2025-03-25T15:00:34.9075173Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "f4b30d6f-9264-460a-a122-e48064e5a0d6",
                            "queued_time": "2025-03-25T15:00:34.9062969Z",
                            "session_id": "c8f3477c-cd91-404a-91e9-c547f8e1e160",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 6,
                            "statement_ids": [
                                6
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, c8f3477c-cd91-404a-91e9-c547f8e1e160, 6, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[I@6b4d7327\n",
                        "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[54] at parallelize at <console>:33\n",
                        "results1: Array[Int] = Array(3, 3, 3, 3, 4, 3, 3, 3, 3, 4)\n"
                    ]
                }
            ],
            "source": [
                "// Run a simple job.\n",
                "// In the resource usage tab, we can see that the job did not use all of available cores. It means that we should increase parallelism to fully use the executor resource.\n",
                "val rdd1 = sc.parallelize(data, 10)\n",
                "val results1 = rdd1.mapPartitions(calculation).collect()\n",
                "println(results1)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2fda60d2-fe24-4f6f-86b8-80df3f42862b",
            "metadata": {
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "##### **Step 3: Now, run the same job with a higher number of partitions (32) to take advantage of all available cores.**\n",
                "You will see that all cores are used."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "ad68c3b4-6695-48d5-afeb-531a9f65ca44",
            "metadata": {
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-03-25T15:04:38.0212916Z",
                            "execution_start_time": "2025-03-25T15:03:56.4600779Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "49fd1fbd-c632-46f1-b57c-22b68a531669",
                            "queued_time": "2025-03-25T15:03:56.4589404Z",
                            "session_id": "c8f3477c-cd91-404a-91e9-c547f8e1e160",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 7,
                            "statement_ids": [
                                7
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, c8f3477c-cd91-404a-91e9-c547f8e1e160, 7, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[I@598ef685\n",
                        "rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[56] at parallelize at <console>:33\n",
                        "results2: Array[Int] = Array(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n"
                    ]
                }
            ],
            "source": [
                "// Run the same job again, but with more partitions.\n",
                "// Now we can see that all cores are used.\n",
                "val rdd2 = sc.parallelize(data, 32)\n",
                "val results2 = rdd2.mapPartitions(calculation).collect()\n",
                "println(results2)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5606f2e5-26ca-4e61-8c02-b026ed2f0603",
            "metadata": {
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### 3.3.4 Access Spark Real-Time Logs  \n",
                "\n",
                "Need to debug issues quickly? **Contextual Monitoring** brings **Spark logs** directly into your notebook! üõ†Ô∏è  \n",
                "\n",
                "- Easily locate **exceptions, warnings, and errors**  \n",
                "- **Search or filter logs** for faster troubleshooting  \n",
                "- Click on the **Log tab** in the cell to explore logs in real-time  \n",
                "\n",
                "![Real-Time Logs](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-3-scheduling-monitoring-debugging/_media/resource.png?raw=true)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3aee0bf8-552e-4dd2-9176-306fb3b12b3e",
            "metadata": {
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 3.5 OSS based Spark UI/Spark History Server\n",
                "\n",
                "Use extended Apache Spark history server to debug and diagnose Apache Spark applications.\n",
                "\n",
                "#### 3.5.1 Open the Spark web UI from progress indicator notebook\n",
                "When an Apache Spark job is triggered, the button to open Spark web UI is inside the More action option in the progress indicator. In cell #5 that you just ran, select Spark web UI on the right and wait for a few seconds, then the Spark UI page appears.\n",
                "![](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-3-scheduling-monitoring-debugging/_media/spark-web-ui-in-the-progress-indicator-notebook.png?raw=true)\n",
                "\n",
                "#### 3.5.2 Open the Spark web UI from Apache Spark application detail page\n",
                "The Spark web UI can also be opened through the Apache Spark application detail page. Select Monitor on the left side of the page, and then select an Apache Spark application. The detail page of the application appears.\n",
                "![](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-3-scheduling-monitoring-debugging/_media/spark-web-ui-from-application-detail-page.png?raw=true)\n",
                "\n",
                "#### 3.5.3 Advanced enhancement on Spark UI/Spark History Server\n",
                "[Graph tab in Apache Spark history server](https://learn.microsoft.com/en-us/fabric/data-engineering/media/apache-spark-history-server/apache-spark-graph-job-id.png)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "79b445f3-76e2-4c10-848f-c46d0ba8af73",
            "metadata": {
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 3.6 Run series\n",
                "The Apache Spark run series automatically categorizes your Spark applications based on recurring pipeline activities, manual notebook runs, or Spark job runs from the same notebook or Spark job definition.\n",
                "\n",
                "The run series feature illustrates the duration trend and data input or output trend for each Spark application instance. It automatically scans the run series, detects anomalies, and provides detailed views for individual Spark applications.\n",
                "\n",
                "You can access the monitor run series feature from the **Monitoring hub**'s historical view and **Recent runs** panel:\n",
                "|![](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-3-scheduling-monitoring-debugging/_media/access-run-series-from-historical-view.png?raw=true)\n",
                "![](./access-run-series-from-recent-run.png)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4d90ce50-8705-4430-95b7-9fa60f7a9730",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 3.7 Spark advisor\n",
                "The Apache Spark advisor analyzes commands and code run by Apache Spark and displays real-time advice for Notebook runs. The Apache Spark advisor has built-in patterns to help users avoid common mistakes. It offers recommendations for code optimization, performs error analysis, and locates the root cause of failures.\n",
                "\n",
                "![](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-3-scheduling-monitoring-debugging/_media/errors.png?raw=true)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "35837f11-dcbf-4e8d-8b2d-a94e26e75c16",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### 3.7.1. Error Type"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "54545f5d-b0a7-4db4-9c1d-19ce19b0d205",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "Advice with Error type usually would be within the cell with failed execution. But not every failed cell will have an error type advice.\n",
                "For this case, usually the tsg will include how you could make your cell execute successfully."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "114ad80b-8292-4887-a9ac-baf40cf7833f",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "##### 3.7.1a Spark Job Failed/File Not Found"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "e1dbccf6-8cdd-4d1c-8ddd-703d9869ad84",
            "metadata": {
                "advisor": {
                    "adviceMetadata": "{\"artifactId\":\"5d820b80-3c7b-41a8-9794-680e3e979597\",\"activityId\":\"a86d7c03-092e-482f-b3ab-ccd21d3b0522\",\"applicationId\":\"application_1740466400982_0001\",\"jobGroupId\":\"10\",\"advices\":{\"error\":2}}"
                },
                "collapsed": false,
                "jupyter": {
                    "outputs_hidden": true
                },
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-02-25T07:10:31.9561393Z",
                            "execution_start_time": "2025-02-25T07:10:27.9126974Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "a9de2d29-638c-42d9-b2ab-076eaa396927",
                            "queued_time": "2025-02-25T07:10:27.9109992Z",
                            "session_id": "a86d7c03-092e-482f-b3ab-ccd21d3b0522",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 10,
                            "statement_ids": [
                                10
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, a86d7c03-092e-482f-b3ab-ccd21d3b0522, 10, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.synapse.widget-view+json": {
                            "widget_id": "cf249f21-38a4-469b-aaa6-b2bd468fa381",
                            "widget_type": "Synapse.DataFrame"
                        },
                        "text/plain": [
                            "SynapseWidget(Synapse.DataFrame, cf249f21-38a4-469b-aaa6-b2bd468fa381)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "ename": "Error",
                    "evalue": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 23.0 failed 4 times, most recent failure: Lost task 1.3 in stage 23.0 (TID 69) (vm-65011780 executor 2): java.io.FileNotFoundException: Not found issues",
                    "output_type": "error",
                    "traceback": [
                        "org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 23.0 failed 4 times, most recent failure: Lost task 1.3 in stage 23.0 (TID 69) (vm-65011780 executor 2): java.io.FileNotFoundException: Not found issues",
                        "\tat $default$anonfun$test$1(<console>:34)\n",
                        "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
                        "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
                        "\tat org.apache.spark.util.Iterators$.size(Iterators.scala:31)\n",
                        "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1953)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1276)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1276)\n",
                        "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2458)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
                        "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:574)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:577)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
                        "\n",
                        "Driver stacktrace:\n",
                        "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2871)\n",
                        "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2807)\n",
                        "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2806)\n",
                        "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
                        "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
                        "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
                        "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2806)\n",
                        "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1229)\n",
                        "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1229)\n",
                        "  at scala.Option.foreach(Option.scala:407)\n",
                        "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1229)\n",
                        "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3070)\n",
                        "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3009)\n",
                        "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2998)\n",
                        "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
                        "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:988)\n",
                        "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2418)\n",
                        "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2439)\n",
                        "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n",
                        "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2483)\n",
                        "  at org.apache.spark.rdd.RDD.count(RDD.scala:1276)\n",
                        "  at $defaulttest(<console>:31)\n",
                        "  ... 53 elided\n",
                        "Caused by: java.io.FileNotFoundException: Not found issues\n",
                        "  at $default$anonfun$test$1(<console>:34)\n",
                        "  at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
                        "  at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
                        "  at org.apache.spark.util.Iterators$.size(Iterators.scala:31)\n",
                        "  at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1953)\n",
                        "  at org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1276)\n",
                        "  at org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1276)\n",
                        "  at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2458)\n",
                        "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
                        "  at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
                        "  at org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
                        "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:574)\n",
                        "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
                        "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:577)\n",
                        "  ... 3 more\n"
                    ]
                }
            ],
            "source": [
                "%%spark\n",
                "def test(): Unit = {\n",
                "    val data = 1 to 100000\n",
                "    val inputRdd = sc.parallelize(data, 3)\n",
                "    inputRdd.map(number => {\n",
                "        // Thread.sleep(1000)\n",
                "        if (number % 5 == 0) {\n",
                "            throw new java.io.FileNotFoundException(\"Not found issues\")\n",
                "        }\n",
                "\n",
                "        if (number % 3 == 0) {\n",
                "            throw new RuntimeException(\"Authenticate failure issues\")\n",
                "        }\n",
                "    }).count()\n",
                "}\n",
                "\n",
                "val df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/online_retail.csv\")\n",
                "display(df)\n",
                "\n",
                "test()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8496983a-d451-4428-9a0b-be0948587f96",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "##### 3.7.1b Path Already exists Error, and solution is provided as: To overwrite the existing file, set mode(\"overwrite\") on the DataFrameWriter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "f2a028d8-afca-433d-9493-7a768e31d564",
            "metadata": {
                "advisor": {
                    "adviceMetadata": "{\"artifactId\":\"5d820b80-3c7b-41a8-9794-680e3e979597\",\"activityId\":\"a86d7c03-092e-482f-b3ab-ccd21d3b0522\",\"applicationId\":\"application_1740466400982_0001\",\"jobGroupId\":\"12\",\"advices\":{\"error\":1}}"
                },
                "jupyter": {
                    "outputs_hidden": true,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-02-25T07:11:24.9535941Z",
                            "execution_start_time": "2025-02-25T07:11:24.0020479Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "0bc621cb-8900-450a-9dfe-f42e07e9909b",
                            "queued_time": "2025-02-25T07:11:24.0006775Z",
                            "session_id": "a86d7c03-092e-482f-b3ab-ccd21d3b0522",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 12,
                            "statement_ids": [
                                12
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, a86d7c03-092e-482f-b3ab-ccd21d3b0522, 12, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "ename": "Error",
                    "evalue": "org.apache.spark.sql.AnalysisException: [PATH_ALREADY_EXISTS] Path abfss://4cb9b656-c8f8-485e-a151-e81bb913abc8@msit-onelake.dfs.fabric.microsoft.com/eb9433f1-ec4d-4683-89bd-49b5c4ee819b/Files/catalogparquet2csv already exists. Set mode as \"overwrite\" to overwrite the existing path.",
                    "output_type": "error",
                    "traceback": [
                        "org.apache.spark.sql.AnalysisException: [PATH_ALREADY_EXISTS] Path abfss://4cb9b656-c8f8-485e-a151-e81bb913abc8@msit-onelake.dfs.fabric.microsoft.com/eb9433f1-ec4d-4683-89bd-49b5c4ee819b/Files/catalogparquet2csv already exists. Set mode as \"overwrite\" to overwrite the existing path.",
                        "  at org.apache.spark.sql.errors.QueryCompilationErrors$.outputPathAlreadyExistsError(QueryCompilationErrors.scala:1596)\n",
                        "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:123)\n",
                        "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
                        "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
                        "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
                        "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:152)\n",
                        "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:125)\n",
                        "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:214)\n",
                        "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:100)\n",
                        "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
                        "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:67)\n",
                        "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:152)\n",
                        "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:145)\n",
                        "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
                        "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
                        "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
                        "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
                        "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
                        "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
                        "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
                        "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
                        "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
                        "  at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:145)\n",
                        "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:129)\n",
                        "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:123)\n",
                        "  at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:200)\n",
                        "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:897)\n",
                        "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n",
                        "  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:379)\n",
                        "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:241)\n",
                        "  at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:888)\n",
                        "  ... 53 elided\n"
                    ]
                }
            ],
            "source": [
                "%%spark\n",
                "df.write.csv(\"Files/catalogparquet2csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7b1e9443-7d2d-4cfb-992d-da6252b2aa7b",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "##### 3.7.1c After adding overwrite as saveMode, execution succeeded."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "9326c5ab-caf7-4115-82b1-ec6a4b396f29",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-02-25T07:12:18.1431603Z",
                            "execution_start_time": "2025-02-25T07:12:14.4199638Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "a2b2254a-58cb-46b2-af40-5a235c4b3c8d",
                            "queued_time": "2025-02-25T07:12:14.4182957Z",
                            "session_id": "a86d7c03-092e-482f-b3ab-ccd21d3b0522",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 13,
                            "statement_ids": [
                                13
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, a86d7c03-092e-482f-b3ab-ccd21d3b0522, 13, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "import org.apache.spark.sql.SaveMode\n"
                    ]
                }
            ],
            "source": [
                "%%spark\n",
                "import org.apache.spark.sql.SaveMode\n",
                "df.write.mode(SaveMode.Overwrite).csv(\"Files/catalogparquet2csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ac7e5c4c-a502-4347-9d6a-3c8287332afe",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 3.7.2. Warning Type"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a4756c2d-9838-417f-aa98-33d23df0c9a3",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "##### Advice with warning type usually would not result in a failed execution, but some of the passed in confs/plugins might not work as expected. Or there might exist perf issue like skew.\n",
                "The message would mostly about how to make it work as you expected"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c7041efa-82fa-4410-a3b8-a84c728c55b7",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "##### 3.7.2a Data Skew/Time Skew"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d6bf2ebc-9ff4-432f-8ac0-2dd049a3b374",
            "metadata": {
                "advisor": {
                    "adviceMetadata": "{\"artifactId\":\"5d820b80-3c7b-41a8-9794-680e3e979597\",\"activityId\":\"a86d7c03-092e-482f-b3ab-ccd21d3b0522\",\"applicationId\":\"application_1740466400982_0001\",\"jobGroupId\":\"14\",\"advices\":{\"warn\":2}}"
                },
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-02-25T07:13:58.7185706Z",
                            "execution_start_time": "2025-02-25T07:13:19.0285811Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "4e7a7515-b629-419e-a8e1-6702048e0b8d",
                            "queued_time": "2025-02-25T07:13:19.026979Z",
                            "session_id": "a86d7c03-092e-482f-b3ab-ccd21d3b0522",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 14,
                            "statement_ids": [
                                14
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, a86d7c03-092e-482f-b3ab-ccd21d3b0522, 14, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2398241\n",
                        "2398279\n",
                        "import org.apache.spark.SparkContext\n",
                        "import scala.util.Random\n",
                        "testDataSkew: (sc: org.apache.spark.SparkContext)Unit\n"
                    ]
                }
            ],
            "source": [
                "%%spark\n",
                "import org.apache.spark.SparkContext \n",
                "import scala.util.Random \n",
                "def testDataSkew(sc: SparkContext): Unit = {\n",
                "    val numMappers = 400\n",
                "    val numKVPairs = 10000\n",
                "    val valSize = 256\n",
                "    val numReducers = 200\n",
                "    val biasPct = 0.4\n",
                "    val biasCount = numKVPairs * biasPct\n",
                "    for (i <- 1 to 2) {\n",
                "      val query = sc.parallelize(0 until numMappers, numMappers).flatMap { p =>\n",
                "        val ranGen = new Random\n",
                "        val arr1 = new Array[(Int, Array[Byte])](numKVPairs)\n",
                "        for (i <- 0 until numKVPairs) {\n",
                "          val byteArr = new Array[Byte](valSize)\n",
                "          ranGen.nextBytes(byteArr)\n",
                "          var key = ranGen.nextInt(Int.MaxValue)\n",
                "          if(i <= biasCount) {\n",
                "            key = 1\n",
                "          }\n",
                "          arr1(i) = (key, byteArr)\n",
                "        }\n",
                "        arr1\n",
                "      }.groupByKey(numReducers)\n",
                "      // Enforce that everything has been calculated and in cache\n",
                "      // scalastyle:off println\n",
                "      println(query.count())\n",
                "      // scalastyle:on println\n",
                "      Thread.sleep(1000)\n",
                "    }\n",
                "  }\n",
                "\n",
                "\n",
                "testDataSkew(sc)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d3507ebf-50c4-4224-9031-688e1ddd2fbf",
            "metadata": {
                "advisor": {
                    "adviceMetadata": "{\"artifactId\":\"5d820b80-3c7b-41a8-9794-680e3e979597\",\"activityId\":\"a86d7c03-092e-482f-b3ab-ccd21d3b0522\",\"applicationId\":\"application_1740466400982_0001\",\"jobGroupId\":\"26\",\"advices\":{\"warn\":1}}"
                },
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-02-25T08:03:27.1195437Z",
                            "execution_start_time": "2025-02-25T08:00:40.8019505Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "98eb2f65-16d6-450a-8784-41e4c3719b3c",
                            "queued_time": "2025-02-25T08:00:40.800386Z",
                            "session_id": "a86d7c03-092e-482f-b3ab-ccd21d3b0522",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 26,
                            "statement_ids": [
                                26
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, a86d7c03-092e-482f-b3ab-ccd21d3b0522, 26, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "import org.apache.spark.TaskContext\n",
                        "res: org.apache.spark.sql.Dataset[Long] = [value: bigint]\n",
                        "res46: Long = 1000000\n"
                    ]
                }
            ],
            "source": [
                "%%spark\n",
                "import org.apache.spark.TaskContext\n",
                "val res = spark.range(0, 1000 * 1000, 1).repartition(200).map { x =>\n",
                "  if (TaskContext.get.attemptNumber == 0 && TaskContext.get.partitionId < 8) {\n",
                "    Thread.sleep(30)\n",
                "  }\n",
                "  x\n",
                "}.repartition(200).map { x =>\n",
                "  x\n",
                "}\n",
                "res.distinct().count()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a3c9c7cf-823a-48d6-b5c0-699a97a66792",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "##### 3.7.2b Hint related warning"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "abac2b6e-3e3f-4e56-85bb-f8fa547268d5",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "##### Hint Not recognized"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9a939060-bcf1-493b-9de4-380d7cf0dd8e",
            "metadata": {
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "create below table first:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d2c9ad9f-057a-4046-8f48-8f9aba620677",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": true,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-02-25T07:17:07.6587092Z",
                            "execution_start_time": "2025-02-25T07:17:06.7222207Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "81d07d1e-115f-4005-9739-8ade487bf634",
                            "queued_time": "2025-02-25T07:16:51.0572906Z",
                            "session_id": "a86d7c03-092e-482f-b3ab-ccd21d3b0522",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 16,
                            "statement_ids": [
                                16
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, a86d7c03-092e-482f-b3ab-ccd21d3b0522, 16, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "ename": "Error",
                    "evalue": "org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException: [TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create table or view `spark_catalog`.`monitoringlh`.`t1` because it already exists.",
                    "output_type": "error",
                    "traceback": [
                        "org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException: [TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create table or view `spark_catalog`.`monitoringlh`.`t1` because it already exists.",
                        "Choose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects.\n",
                        "  at org.apache.spark.sql.errors.QueryCompilationErrors$.tableAlreadyExistsError(QueryCompilationErrors.scala:2382)\n",
                        "  at org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:57)\n",
                        "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
                        "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
                        "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
                        "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:152)\n",
                        "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:125)\n",
                        "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:214)\n",
                        "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:100)\n",
                        "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
                        "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:67)\n",
                        "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:152)\n",
                        "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:145)\n",
                        "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
                        "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
                        "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
                        "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
                        "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
                        "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
                        "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
                        "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
                        "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
                        "  at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:145)\n",
                        "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:129)\n",
                        "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:123)\n",
                        "  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n",
                        "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n",
                        "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
                        "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n",
                        "  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:640)\n",
                        "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
                        "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:630)\n",
                        "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:671)\n",
                        "  ... 54 elided\n"
                    ]
                }
            ],
            "source": [
                "spark.sql(\"CREATE TABLE t1 (str STRING) USING parquet\")\n",
                "spark.sql(\"CREATE TABLE t2 (str STRING) USING parquet\")\n",
                "spark.sql(\"CREATE TABLE t3 (str STRING) USING parquet\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6b9998fa-3b6e-4468-9d53-515270a6c941",
            "metadata": {
                "advisor": {
                    "adviceMetadata": "{\"artifactId\":\"5d820b80-3c7b-41a8-9794-680e3e979597\",\"activityId\":\"a86d7c03-092e-482f-b3ab-ccd21d3b0522\",\"applicationId\":\"application_1740466400982_0001\",\"jobGroupId\":\"17\",\"advices\":{\"warn\":1}}"
                },
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-02-25T07:18:05.5140427Z",
                            "execution_start_time": "2025-02-25T07:18:03.8036464Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "b0d1399a-cdae-4ed1-b3f8-9fe3c6eca0ee",
                            "queued_time": "2025-02-25T07:18:03.8021474Z",
                            "session_id": "a86d7c03-092e-482f-b3ab-ccd21d3b0522",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 17,
                            "statement_ids": [
                                17
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, a86d7c03-092e-482f-b3ab-ccd21d3b0522, 17, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "res30: org.apache.spark.sql.DataFrame = [str: string]\n"
                    ]
                }
            ],
            "source": [
                "spark.sql(\"SELECT /*+ testtest */ * FROM t1\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "18fc3f48-2fb8-4e42-a562-7770ad3963ac",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "##### Hint Overriden"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f7616764-8625-4d64-a4fa-4affdd386965",
            "metadata": {
                "advisor": {
                    "adviceMetadata": "{\"artifactId\":\"5d820b80-3c7b-41a8-9794-680e3e979597\",\"activityId\":\"a86d7c03-092e-482f-b3ab-ccd21d3b0522\",\"applicationId\":\"application_1740466400982_0001\",\"jobGroupId\":\"18\",\"advices\":{\"warn\":1}}"
                },
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-02-25T07:18:15.3160656Z",
                            "execution_start_time": "2025-02-25T07:18:13.6538234Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "98f96815-9783-4260-83b0-56d0fae7f39d",
                            "queued_time": "2025-02-25T07:18:13.6523037Z",
                            "session_id": "a86d7c03-092e-482f-b3ab-ccd21d3b0522",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 18,
                            "statement_ids": [
                                18
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, a86d7c03-092e-482f-b3ab-ccd21d3b0522, 18, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "res31: org.apache.spark.sql.DataFrame = [str: string, str: string]\n"
                    ]
                }
            ],
            "source": [
                "spark.sql(\"SELECT /*+ BROADCAST(t1), MERGE(t1, t2) */ * FROM t1 INNER JOIN t2 ON t1.str = t2.str\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "83e29243-fdfe-4b38-aaf4-5a1ecf7a59cd",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "##### Couldn't found relations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "588508ce-0573-4cef-9b6d-efe6cb7ae730",
            "metadata": {
                "advisor": {
                    "adviceMetadata": "{\"artifactId\":\"5d820b80-3c7b-41a8-9794-680e3e979597\",\"activityId\":\"a86d7c03-092e-482f-b3ab-ccd21d3b0522\",\"applicationId\":\"application_1740466400982_0001\",\"jobGroupId\":\"19\",\"advices\":{\"warn\":1}}"
                },
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-02-25T07:18:26.5202558Z",
                            "execution_start_time": "2025-02-25T07:18:24.8388111Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "c7fb121d-f3ac-4419-8a00-bbb2164b6e17",
                            "queued_time": "2025-02-25T07:18:24.8374303Z",
                            "session_id": "a86d7c03-092e-482f-b3ab-ccd21d3b0522",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 19,
                            "statement_ids": [
                                19
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, a86d7c03-092e-482f-b3ab-ccd21d3b0522, 19, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "res32: org.apache.spark.sql.DataFrame = [str: string, str: string]\n"
                    ]
                }
            ],
            "source": [
                "spark.sql(\"SELECT /*+ BROADCAST(test1) */ * FROM t1 INNER JOIN t2 ON t1.str = t2.str\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d32b1fb6-1744-4a03-8e17-95439c1aa876",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### 3.7.3. Info Type"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "913e0eee-6e31-4723-bc51-a6ffa44255a7",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "##### Advice with Info type usually mean use this way might improve the execution."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8adea6ec-0b5a-4c1e-8d17-c4432f06dd3d",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "##### 3.7.3a RandomSplit might introduce inconsistency"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "fb8ecf3e-c928-4f2b-b115-468d9f91323a",
            "metadata": {
                "advisor": {
                    "adviceMetadata": "{\"artifactId\":\"5d820b80-3c7b-41a8-9794-680e3e979597\",\"activityId\":\"a86d7c03-092e-482f-b3ab-ccd21d3b0522\",\"applicationId\":\"application_1740466400982_0001\",\"jobGroupId\":\"20\",\"advices\":{\"info\":1}}"
                },
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-02-25T07:18:36.1800238Z",
                            "execution_start_time": "2025-02-25T07:18:34.5245176Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "e708fd25-f510-4e2b-b84f-9bdf9eb61cba",
                            "queued_time": "2025-02-25T07:18:34.5228456Z",
                            "session_id": "a86d7c03-092e-482f-b3ab-ccd21d3b0522",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 20,
                            "statement_ids": [
                                20
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, a86d7c03-092e-482f-b3ab-ccd21d3b0522, 20, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[152] at parallelize at <console>:31\n",
                        "rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[156] at repartition at <console>:31\n",
                        "train: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[157] at randomSplit at <console>:32\n",
                        "test: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[158] at randomSplit at <console>:32\n",
                        "res33: Array[Int] = Array(2, 3, 4, 5, 6, 8, 9, 10, 13, 14)\n"
                    ]
                }
            ],
            "source": [
                "%%spark\n",
                "val rdd = sc.parallelize(1 to 1000000)\n",
                "val rdd2 = rdd.repartition(64)\n",
                "val Array(train, test) = rdd2.randomSplit(Array(70, 30), 1)\n",
                "train.takeOrdered(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bfd0294c-2f61-49de-a12a-b2749b6bfba6",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### below data is not stable during multi runs, even there is overlap between dataset: train and test."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "5dfe3058-f2a6-49ea-be3c-469f2305ef48",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-02-25T07:27:58.9452472Z",
                            "execution_start_time": "2025-02-25T07:27:57.960213Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "50246ba1-5d86-4d7d-a235-46d4c8629e75",
                            "queued_time": "2025-02-25T07:27:57.9586626Z",
                            "session_id": "a86d7c03-092e-482f-b3ab-ccd21d3b0522",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 21,
                            "statement_ids": [
                                21
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, a86d7c03-092e-482f-b3ab-ccd21d3b0522, 21, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "res34: Array[Int] = Array(2, 3, 4, 5, 6, 8, 9, 10, 13, 14)\n"
                    ]
                }
            ],
            "source": [
                "%%spark\n",
                "train.takeOrdered(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9bd95e0d-2452-4479-a27f-bed5401e9c1f",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "445494eb-0fd1-4a01-b26a-fbaefdb1368c",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "##### 3.7.3b DivisionExprAdvise"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "708c8012-b0a3-46f0-a3ee-b1bb8b75336b",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "Division expressions can be reduced and shuffled to reduce rounding error propagation. \n",
                "\n",
                "In this advise, we check the optimized logical plan to get the expressions below: \n",
                "\n",
                "A / B * C, A * (B / C), A / B / C, A / (B / C) and if A‚Äôs data type is Double. \n",
                "\n",
                "These expressions can be converted to sematic equivalent expressions: Like A / B / C into A / (B * C) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "2f8531ff-dee1-425f-90d6-dabcfb7de389",
            "metadata": {
                "advisor": {
                    "adviceMetadata": "{\"artifactId\":\"5d820b80-3c7b-41a8-9794-680e3e979597\",\"activityId\":\"a86d7c03-092e-482f-b3ab-ccd21d3b0522\",\"applicationId\":\"application_1740466400982_0001\",\"jobGroupId\":\"22\",\"advices\":{\"info\":1}}"
                },
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-02-25T07:28:19.7800819Z",
                            "execution_start_time": "2025-02-25T07:28:18.8783611Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "90c12bb6-90ce-42fa-a478-99f97193a21c",
                            "queued_time": "2025-02-25T07:28:18.8767936Z",
                            "session_id": "a86d7c03-092e-482f-b3ab-ccd21d3b0522",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 22,
                            "statement_ids": [
                                22
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, a86d7c03-092e-482f-b3ab-ccd21d3b0522, 22, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "exprA: String = (CAST(id AS DOUBLE) + 1.0D)\n",
                        "exprB: String = (CAST(id AS DOUBLE) + 2.0D)\n",
                        "exprC: String = (CAST(id AS DOUBLE) * 3.0D)\n",
                        "expr: String = (CAST(id AS DOUBLE) + 1.0D) / (CAST(id AS DOUBLE) + 2.0D) * (CAST(id AS DOUBLE) * 3.0D)\n",
                        "df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [(((CAST(id AS DOUBLE) + 1.0) / (CAST(id AS DOUBLE) + 2.0)) * (CAST(id AS DOUBLE) * 3.0)): double]\n",
                        "res35: Array[org.apache.spark.sql.Row] = Array([0.0])\n"
                    ]
                }
            ],
            "source": [
                "val exprA = \"(CAST(id AS DOUBLE) + 1.0D)\"\n",
                "val exprB = \"(CAST(id AS DOUBLE) + 2.0D)\"\n",
                "val exprC = \"(CAST(id AS DOUBLE) * 3.0D)\"\n",
                "val expr = s\"${exprA} / ${exprB} * ${exprC}\"\n",
                "val df = spark.range(1)\n",
                "        .selectExpr(expr)\n",
                "        .as(\"result\")\n",
                "      df.collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e1643d11",
            "metadata": {},
            "source": [
                "### 3.8 Spark job definition inline monitoring\n",
                "\n",
                "The Spark job definition Inline Monitoring feature allows you to view Spark job definition submission and run status in real-time, as well as view the Spark job definition's past runs and configurations. You can navigate to the Spark application detail page to view more details.\n",
                "\n",
                "#### 3.8.1 Spark job definition inline monitoring\n",
                "The Spark job definition inline monitoring feature allows you to view Spark job definition submission and run status in real-time. You can also view the Spark job definition's past runs and configurations and navigate to the Spark application detail page to view more details.\n",
                "![](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-3-scheduling-monitoring-debugging/_media/spark-job-definition-inline-monitoring.png?raw=true)\n",
                "\n",
                "\n",
                "#### 3.8.2 Spark job definition item view in workspace\n",
                "You can access the job runs associated with specific Spark job definition items by using the Recent runs contextual menu on the workspace homepage.\n",
                "![](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-3-scheduling-monitoring-debugging/_media/spark-job-definition-artifact-view-in-workspace.png?raw=true)\n",
                "\n",
                "\n",
                "### 3.9 Pipeline Spark activity inline monitoring\n",
                "For Pipeline Spark Activity Inline Monitoring, deep links have been built into the Notebook and Spark job definition activities within the Pipeline. You can view Spark application execution details, the respective Notebook and Spark job definition snapshot, and access Spark logs for troubleshooting. If the Spark activities fail, the inline error message is also available within Pipeline Spark activities.\n",
                "\n",
                "[Snapshot](https://learn.microsoft.com/en-us/fabric/data-engineering/media/spark-detail-monitoring/related-items.png)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "092baecd-f977-427e-8eeb-b90bedb8921b",
            "metadata": {
                "microsoft": {
                    "language": "scala",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 3.10 Collect log with Azure log analystics, Azure Storage account and Azure eventhub\n",
                "\n",
                "The Fabric Apache Spark diagnostic emitter extension is a library that enables Apache Spark applications to emit logs, event logs, and metrics to multiple destinations, including Azure log analytics, Azure storage, and Azure event hubs.\n",
                "\n",
                "#### 3.10.1 Configure with Log Analytics Workspace ID and Key\n",
                "\n",
                "```\n",
                "spark.synapse.diagnostic.emitters: <EMITTER_NAME>\n",
                "spark.synapse.diagnostic.emitter.<EMITTER_NAME>.type: \"AzureLogAnalytics\"\n",
                "spark.synapse.diagnostic.emitter.<EMITTER_NAME>.categories: \"Log,EventLog,Metrics\"\n",
                "spark.synapse.diagnostic.emitter.<EMITTER_NAME>.workspaceId: <LOG_ANALYTICS_WORKSPACE_ID>\n",
                "spark.synapse.diagnostic.emitter.<EMITTER_NAME>.secret: <LOG_ANALYTICS_WORKSPACE_KEY>\n",
                "spark.fabric.pools.skipStarterPools: \"true\" //Add this Spark property when using the default pool.\n",
                "```\n",
                "\n",
                "#### 3.10.2 Configure with Azure Storage URI and Access key\n",
                "\n",
                "```\n",
                "spark.synapse.diagnostic.emitters: MyStorageBlob\n",
                "spark.synapse.diagnostic.emitter.MyStorageBlob.type: \"AzureStorage\"\n",
                "spark.synapse.diagnostic.emitter.MyStorageBlob.categories: \"DriverLog,ExecutorLog,EventLog,Metrics\"\n",
                "spark.synapse.diagnostic.emitter.MyStorageBlob.uri:  \"https://<my-blob-storage>.blob.core.windows.net/<container-name>/<folder-name>\"\n",
                "spark.synapse.diagnostic.emitter.MyStorageBlob.auth: \"AccessKey\"\n",
                "spark.synapse.diagnostic.emitter.MyStorageBlob.secret: <storage-access-key>\n",
                "spark.fabric.pools.skipStarterPools: \"true\" //Add this Spark property when using the default pool.\n",
                "```\n",
                "\n",
                "#### 3.10.3 Configure with Azure Event Hubs Connection String\n",
                "\n",
                "```\n",
                "spark.synapse.diagnostic.emitters: MyEventHub\n",
                "spark.synapse.diagnostic.emitter.MyEventHub.type: \"AzureEventHub\"\n",
                "spark.synapse.diagnostic.emitter.MyEventHub.categories: \"Log,EventLog,Metrics\"\n",
                "spark.synapse.diagnostic.emitter.MyEventHub.secret: <connection-string>\n",
                "spark.fabric.pools.skipStarterPools: \"true\" //Add this Spark property when using the default pool.\n",
                "```\n"
            ]
        }
    ],
    "metadata": {
        "a365ComputeOptions": null,
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "kernelspec": {
            "display_name": "Synapse PySpark",
            "language": "Python",
            "name": "synapse_pyspark"
        },
        "language_info": {
            "name": "python"
        },
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "sessionKeepAliveTimeout": 0,
        "synapse_widget": {
            "state": {},
            "version": "0.1"
        },
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1800000"
                }
            }
        },
        "dependencies": {
            "environment": {},
            "lakehouse": {}
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}